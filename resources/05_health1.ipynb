{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eeaee52-73c6-4064-a842-f1438dab0264",
   "metadata": {},
   "source": [
    "# Exploring Health Data with KNN\n",
    "\n",
    "**Complete by: Thursday 24 Oct. at 10:55am**  \n",
    "Data: <https://jrladd.com/CIS241/data/500_Cities_small.csv>\n",
    "\n",
    "What does it mean for a *place* to be healthy? What can we learn about the overall health of people in the United States by looking at health by city and locality? These are the questions that the CDC's [PLACES](https://www.cdc.gov/places/about/index.html) project attempts to answer. This project originally began in 2015 as the \"500 Cities Project\": an annual survey of the 500 largest cities in the US. By surveying these cities every year, the CDC hopes to better understand long-term health among Americans all across the country.\n",
    "\n",
    "As we continue through the COVID-19 pandemic, data analysis focused on health and healthcare continues to be at the center of public conversation. Good data analysis in this domain is essential for public health in the 21st century. In this week's workshop, you'll examine recent data from the PLACES project to better understand American's overall health.\n",
    "\n",
    "***For this workshop, you will make a coherent and readable report. As you go through each section, make sure you are explaining, describing and interpreting all code, visualizations, and statistical output. The goal is that a reader with no previous knowledge of the data should be able to understand your findings.***\n",
    "\n",
    "Your report should have the following sections:\n",
    "\n",
    "## Data Wrangling and Exploration\n",
    "\n",
    "- Import the necessary libraries. Which parts of sklearn will you need? What won't you need?\n",
    "- Read the data as a CSV using the link above. Take a look at the data, and **describe its rows and columns**.\n",
    "- Go to the website for the [2019 Release of the 500 Cities data](https://dev.socrata.com/foundry/chronicdata.cdc.gov/6vp6-wxuq). (Though this is the 2019 release, much of the data is from 2016-17.) Download the CSV from the site and open it in Excel. How is it similar or different from the data you just loaded into JupyterHub? What are some reasons we can't use the original data in our analysis? **Write a short answer before continuing.**\n",
    "- To deal with this untidy data, I converted it through a process called \"pivoting.\" If you're curious about how I turned the data from the original format into the CSV we are using, you can see the pivoting code below. **You do *not* need to run this code, but in a markdown cell explain line-by-line how this code transformed the data from the website into the data linked at the top of this notebook.**\n",
    "<details>\n",
    "<summary>Click here for the pivot solution</summary>\n",
    "<pre><code>health = (health.pivot_table(index=[\"StateAbbr\",\"CityName\"],\n",
    "                             columns=\"Short_Question_Text\",\n",
    "                             values=\"Data_Value\",\n",
    "                             aggfunc=np.mean)\n",
    "          .reset_index())</code></pre>\n",
    "</details>\n",
    "- **Create a regression plot showing the relationship between two of the measures.** Try to pick variables that you think could be correlated in some way, and explain what you find.\n",
    "- Also **create a box plot showing the distributions of one of the measures in every US state.** Make sure the graph is readable even though it's showing 50 states.\n",
    "- **Interpret both plots fully, accurately, and in terms of the data.**\n",
    "\n",
    "## KNN Regression\n",
    "\n",
    "One possible thing we'd want to know is: how healthy are people in different cities overall? If we had information on a new city, could we predict how healthy the people in that city might be?\n",
    "\n",
    "- Use `Physical Health` as your target variable. This variable does ***not*** indicate that people are healthy. **Write down what this variable actually means.** You can check the PLACES documentation that I linked to above.\n",
    "- Next choose some predictor variables. You can use almost all of the other variables if you like, but give some thought about which ones you should include. (As a shortcut, you can use df.columns to display all the column names and then copy them, instead of typing each one.) *Remember, if you use categorical variables that will generate hundreds of columns in your one-hot encoded data, that will slow down your model and likely cause your kernel to overload!*\n",
    "- **Choose a value for $k$, and explain your choice in terms of the bias-variance tradeoff.** This will require you to define bias and variance and explain how $k$ effects them. You might try the steps below a few times with a few different values for $k$ before you settle on a good one.\n",
    "- Run a KNN regression using these variables and targets. Make sure you've scaled the variables and done all appropriate pre-processing. **Explain your steps and interpret the results fully.**\n",
    "- Use out-of-sample validation to check your model's residuals and calculate some appropriate measures. **How did your model perform?**\n",
    "\n",
    "## KNN Classification\n",
    "\n",
    "Beyond predicting an individual value, we might want to know if the health of people in certain cities varies strongly by state. Given information about a new city, could we correctly predict the state in which it's located?\n",
    "\n",
    "- Use `StateAbbr` as your target. But **there are too many states to reasonably interpret the results**. Let's narrow it down to just the two states with more than 40 cities in our data. **What data wrangling steps would you need to do to find these states? Think of a possible approach and narrow down your data to just these two states.**\n",
    "- Choose some predictor variables. Will you use different ones this time, or the same ones you used for regression?\n",
    "- Run a KNN classification using these variables and targets. Make sure you've scaled variables and done all appropriate pre-processing. **Explain your steps and interpret the results fully.**\n",
    "- Use out-of-sample validation to check your model's confusion matrix, calculate appropriate measures, and make some visualizations. **How did your model perform?**\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Write a brief summary of what you learned about American cities through this analysis. Did you find it easier to predict values or categories with this data? What other approaches might you recommend? Is there additional data on cities that you might want to add to this analyis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8d24f-05bf-4a07-96e1-d82a014a3c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
