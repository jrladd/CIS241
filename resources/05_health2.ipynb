{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f3afd3-82de-436c-8993-04390e9fcfb3",
   "metadata": {},
   "source": [
    "# Modeling Health Data with Random Forest Classification\n",
    "\n",
    "**Complete by: Tuesday 31 Oct. at 10:55am**  \n",
    "Data: <https://jrladd.com/CIS241/data/500_Cities_small.csv>\n",
    "\n",
    "What does it mean for a *place* to be healthy? What can we learn about the overall health of people in the United States by looking at health by city and locality? These are the questions that the CDC's [PLACES](https://www.cdc.gov/places/about/index.html) project attempts to answer. This project originally began in 2015 as the \"500 Cities Project\": an annual survey of the 500 largest cities in the US. By surveying these cities every year, the CDC hopes to better understand long-term health among Americans all across the country.\n",
    "\n",
    "As we continue through the COVID-19 pandemic, data analysis focused on health and healthcare continues to be at the center of public conversation. Good data analysis in this domain is essential for public health in the 21st century. In this week's workshop, you'll examine recent data from the PLACES project to better understand American's overall health.\n",
    "\n",
    "***For this workshop, you will make a coherent and readable report. As you go through each section, make sure you are explaining, describing and interpreting all code, visualizations, and statistical output. The goal is that a reader with no previous knowledge of the data should be able to understand your findings.***\n",
    "\n",
    "\n",
    "## Data Wrangling and Exploration\n",
    "\n",
    "- Import the necessary libraries. Which parts of sklearn will you need? What won't you need?\n",
    "- Read the data as a CSV using the link above. Take a look at the data, and **describe its rows and columns**.\n",
    "- Go to the website for the [2019 Release of the 500 Cities data](https://dev.socrata.com/foundry/chronicdata.cdc.gov/6vp6-wxuq). (Though this is the 2019 release, much of the data is from 2016-17.) Download the CSV from the site and open it in Excel. How is it similar or different from the data you just loaded into JupyterHub? What are some reasons we can't use the original data in our analysis? **Write a short answer before continuing.**\n",
    "- To deal with this untidy data, I converted it through a process called \"pivoting.\" If you're curious about how I turned the data from the original format into the CSV we are using, you can see the pivoting code below.\n",
    "<details>\n",
    "<summary>Click here for the pivot solution</summary>\n",
    "<pre><code>health = (health.pivot_table(index=[\"StateAbbr\",\"CityName\"],\n",
    "                             columns=\"Short_Question_Text\",\n",
    "                             values=\"Data_Value\",\n",
    "                             aggfunc=np.mean)\n",
    "          .reset_index())</code></pre>\n",
    "</details>\n",
    "- **Create a regression plot showing the relationship between two of the measures.** Try to pick variables that you think could be correlated in some way, and explain what you find.\n",
    "- Also **create a box plot showing the distributions of one of the measures in every US state.** To make this readable, check the Seaborn documentation to see if you can make the plot wider.\n",
    "- **Interpret both plots fully, accurately, and in terms of the data.**\n",
    "\n",
    "## Exploring Data with Decision Trees\n",
    "\n",
    "While a random forest approach is best for accurate classification overall, individual decision trees are useful for exploring the data and understanding how variables interact.\n",
    "\n",
    "- Using sklearn's `DecisionTreeClassifier`, fit a decision tree model to the data set. Use `StateAbbr` as your target. But **there are too many states to reasonably interpret the results**. Let's narrow it down to just the two states with more than 40 cities in our data. **What data wrangling steps would you need to do to find these states? Think of a possible approach and narrow down your data to just these two states.**\n",
    "- *Keep in mind all the steps that will go into this: preparing and splitting the data, standardizing the data, and fitting the model.*\n",
    "- Create a visualization of the decision tree. It should look like this one (the Penguins example from Tuesday):\n",
    "\n",
    "![](https://jrladd.com/CIS241/slides/img/decisiontree.png)\n",
    "\n",
    "- Sci-kit learn has a built-in function to make these tree diagrams. Make sure it has colors and labels for each variable, just like the example above. You'll also need to make the figure size larger to make this readable.\n",
    "- (n.b.: Though the example above uses `entropy`, yours will say `gini`. This is showing the Gini impurity measure for each node.)\n",
    "- **Write an interpetation of your decision tree. What does it tell you about the relationships between your variables? Which variables seem most important? Will this help you determine hyperparameters when you fit your random forest classifier?**\n",
    "\n",
    "\n",
    "## Classifying Data with the Random Forest\n",
    "\n",
    "We still want to know if the health of people in certain cities varies strongly by state. Given information about a new city, could we correctly predict the state in which it's located? Let's see if we can get better results with the Random Forest than we did with KNN.\n",
    "\n",
    "- Use `StateAbbr` as your target. But **there are too many states to reasonably interpret the results**. As you did in the last section, narrow it down to just the two states with more than 40 cities in our data.\n",
    "- Choose some predictor variables. Will you use different ones this time, or the same ones you used for the single decision tree?\n",
    "- Run a Random Forest classification using these variables and targets. Make sure you've scaled the variables and done all appropriate pre-processing. **Explain your steps and interpret the results fully.**\n",
    "- Find the variable importance of each predictor. **Interpret these fully. Which variables were most important and why? Did this surprise you or not, based on what you learned from the decision tree?**\n",
    "- Use out-of-sample validation to check your model's confusion matrix, calculate appropriate measures, and make some visualizations. **How well did your model perform?** (Keep in mind that we couldn't do an ROC curve on Tuesday because we didn't have a binary classifier, but *this* classifier will be binary.)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Write a brief summary of what you learned about American cities through this analysis. Did your random forest model perform better than KNN? Why or why not?  What other approaches might you recommend? Is there additional data on cities that you might want to add to this analyis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e64eb-46ac-4a82-9dc0-ed3705bf9217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
