[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CIS241 Intro to Data Science",
    "section": "",
    "text": "0.1 Course Description\nFrom social media to the James Webb telescope, from Shakespeare’s plays to the U.S. Census, data is being collected all around us, all the time. How can we make sense of these near-constant streams of information?\nThis course will attempt to answer this question by introducing the concepts and practices involved in data analysis: data collection and preparation, exploratory analysis, and prediction and classification. Using the programming language Python and other industry-standard tools, we will practice ways of working with data from simple summary statistics to advanced machine learning models. At each step of the way, we’ll discuss how to approach data analysis critically and ethically. And we’ll explore data sets from a wide range of fields and disciplines, including sociology, ecology, business, film, and history.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Homepage</span>"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "CIS241 Intro to Data Science",
    "section": "0.2 Learning Goals",
    "text": "0.2 Learning Goals\nAt the end of this course, you should be able to:\n\nUnderstand and implement the data analysis process, from data collection to communicating results.\nUse exploratory data analysis to quickly understand a complex dataset.\nApply modeling techniques to make predictions.\nEvaluate the effectiveness of different modeling techniques.\nThink and act ethically at all steps of the data analysis process.\n\nBanner image: The Library Computer Access/Retrieval System (LCARS), a fictional operating system and data analysis interface from Star Trek: The Next Generation",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Homepage</span>"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "2  Course Schedule",
    "section": "",
    "text": "2.1 Textbooks:\nThere are no required textbooks for this course. All textbooks are free online or made available to you on Sakai.",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Schedule</span>"
    ]
  },
  {
    "objectID": "schedule.html#textbooks",
    "href": "schedule.html#textbooks",
    "title": "2  Course Schedule",
    "section": "",
    "text": "McKinney, Python for Data Analysis\nUW, Visualization Curriculum for Altair\nWilke, Fundamentals of Data Visualization\nDowney, Elements of Data Science\nBruce, Bruce, and Gedeck, Practical Statistics for Data Scientists 2nd ed.\n\n\n2.1.1 Note on the schedule\nKeep in mind that some of this schedule could change throughout the semester. However, if anything changes I’ll update this page, and I’ll be sure to give you plenty of advance notice.",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Schedule</span>"
    ]
  },
  {
    "objectID": "schedule.html#software",
    "href": "schedule.html#software",
    "title": "2  Course Schedule",
    "section": "2.2 Software",
    "text": "2.2 Software\nAll projects in this course will be scripted and analyzed using Python, an open source programming language and environment. Specifically, we will be using JupyterHub as our programming environment. No previous experience with Python, statistical software packages, or computer programming is required.",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course Schedule</span>"
    ]
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "3  Course Policies",
    "section": "",
    "text": "3.1 Shared Expectations for this Class",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Policies</span>"
    ]
  },
  {
    "objectID": "policies.html#shared-expectations-for-this-class",
    "href": "policies.html#shared-expectations-for-this-class",
    "title": "3  Course Policies",
    "section": "",
    "text": "3.1.1 Preparation and Pacing\nThis class is a broad overview of the field of data analysis, and foundational skills in data and programming. We will cover a lot of information during the semester, and you will have many opportunities to practice these skills, discuss ethical aspects of data mining with your peers, and collaborate on projects. Some of you may be entering the classroom with more advanced prior knowledge of these topics, while others may be encountering these concepts for the first time. Group learning, coding, and discussion are key aspects of this course. This means that we all need to do our part to be prepared for each class, and to foster a positive and inquisitive learning environment.\nIn between classes you should:\n\nReview your notes from class\nRead relevant portions of your textbook or any online readings\nRefer to this site for updates to the schedule\nDO NOT wait until the last minute to begin your final project\nProofread your writing and coding assignments\nEMAIL your instructor with any questions. Ask lots of questions!\n\nEach class must create its own learning community as the result of shared efforts on the part of all members. It is your responsibility as a member of this learning community to help your fellow students by attending class and turning in assignments on time. If you must miss a class or turn in an assignment late, please let me know beforehand so that we may work out a way for you to make up the work. You do not need a doctor’s note or other written excuse, but please let me know if there are special circumstances that may prevent you from completing a large amount of coursework.\n\n\n3.1.2 Attendance & Participation\nGood participation should be understood as consistent and thoughtful contribution to the classroom community, an engagement with course materials and conversations, and a general responsiveness to (and respect for) one’s fellow students and instructor. This isn’t an accounting of how often you speak in class. Instead, it’s about what you offer to the intellectual life of the class, and everyone contributes to this on-going work in different ways. Particularly because so much of the class will consist of hands-on work, you’ll get participation credit simply for attending class and doing the work with care.\nAttending class is not optional: regular attendance is necessary to succeed in this course. Each day will have new content, activities, and learning opportunities. You’ll want to attend as much as possible to avoid falling behind. If you have to miss class for any reason, please let me know in advance, especially if you have to miss more than one class in a row. Missing class is not an excuse for missing assignment deadlines, being unprepared for the next class session, or falling behind on the weekly workshops. You are responsible for keeping up-to-date with the work of the course or for communicating with me about unexpected circumstances which change your ability to do so. You can always reach out to me or a classmate to find out what you missed. We’re all in this together, and I’ll do my best to make sure no one falls behind.\n\n\n3.1.3 Lateness and Extensions\nAll assignments are due by the time listed on the prompt. I know that things don’t always go according to plan. If you need an extension, simply ask for one (using the web form), and you’ll almost always receive it. You don’t need an excuse to receive an extension!\nHowever, you must ask for the extension at least 24 hours before the assignment deadline. To request an extension, simply fill out this form, where you will propose a new deadline for the assignment (a good rule of thumb is a day or two after the original deadline). Any work received late (less than 48 hours) without an agreed-upon extension will receive a point deduction.\nBut keep in mind: I cannot accept any assignment more than 48 hours late. Assignments more than 48 hours late will receive a 0.\n\n3.1.3.1 Instructor Absence\nIf I have to miss class due to illness or any other reason, I will let you know as soon as possible via email. I’ll also post any relevant materials on Sakai or on this website, and my email will let you know what you need to do. It’s a good idea to check your email at least once a day, since that’s the primary way I’ll keep in touch about any changes to the schedule.\n\n\n\n3.1.4 Academic Honesty and Integrity\nIn this course, as in all courses, you are expected to adhere to W&J’s standards of academic honesty and integrity. You should refer to the College Catalog on MyW&J for the details of this policy and how cases of academic misconduct are handled.\nIn general, when completing writing assignments, ensure all work is your own, and give credit where it is due in your citations. Likewise do not collaborate on tests, or homework assignments unless explicitly granted permission. If you have any concerns about whether you might have plagiarized (e.g., if you’re not sure about some particular rule), please get in touch with me immediately—I will gladly discuss the matter with you.\n\n3.1.4.1 Academic Honesty & Coding\nThese standards of academic integrity apply to coding as well as other kinds of classwork. Citation isn’t just for papers! It’s very common, and perfectly permissible, to borrow code snippets from a classmate or from somewhere online. When you do, make sure to include a note in your code’s comments about where the code came from. Keep in mind that this includes the use of any automated writing or code assistants like ChatGPT—any use of such programs should be cited like any website, with a link to the chat record. Not only will this let you give appropriate intellectual credit and avoid plagiarism, it will also help you remember what you were trying to do when you revise your code later.\nWith regard to ChatGPT, Gemini, GitHub Copilot, or any related generative AI systems, keep in mind that in this class you are only permitted to use these systems to help with your code, never with your writing. If you do use a code assistant to help, you must cite what you used in a comment and include a link to the chat record. And keep in mind that these tools may not give you the best advice! It’s always better to work from our course materials or ask a classmate for help.\n\n\n\n3.1.5 Names and Pronouns\nEveryone has the right to be addressed as they feel most comfortable. It’s hard to learn if you cringe every time I call on you or address you in communication. I will ask everyone at the beginning of the semester to tell me their preferred name, pronunciation, and the pronouns they use. I will do my very best to get it right. Please do not feel the need to change your name or pronouns to “make it easy” for me, and please correct me if I get it wrong! This policy goes for everyone—use the names and pronouns dictated by the person to whom they belong. Also, our personal growth does not always align with the semester system: If your name or pronoun changes part way through the class, please send me an update!\n\n\n3.1.6 Class Technology\nPlease be respectful with your use of laptops and technology in class. I request that you only use them for class related purposes, as I and others may find them distracting (For example, no email or social media should be open in your browser tabs!). Cell phones should be kept silent and put away, and you can expect the same from me.\n\n\n3.1.7 Email Response Time\nI encourage you to email me anytime with questions or thoughts about the class. It can sometimes take me up to 24 hours to respond to your message, but I will do my best to get back to you within that period. On the weekends (Friday 5pm–Monday 9am), the response time is 48 hours. This means that if you have a question about an assignment that you email to me the night before something is due, I may not be able to get back to you until after the assignment was due. In these cases, it’s best to plan ahead. But don’t hesitate to email whenever you have a question, and I’ll get back to you as soon as I can!",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Policies</span>"
    ]
  },
  {
    "objectID": "policies.html#important-resources-for-this-class-and-your-college-journey",
    "href": "policies.html#important-resources-for-this-class-and-your-college-journey",
    "title": "3  Course Policies",
    "section": "3.2 Important Resources for this Class and Your College Journey",
    "text": "3.2 Important Resources for this Class and Your College Journey\n\n3.2.1 CIS PAL Tutors\nThe CIS department has its own PAL tutors: they’re available to help Sunday–Thursday evenings from 7–10pm in the CIS Lounge on the second floor of the Tech Center. (I’ll post more detailed tutor bios and schedules on Sakai.)\nAll the CIS PAL tutors are students who’ve taken this class before and/or have lots of knowledge about Python and data analysis. They’re a great resource, and they’re there to help you! I encourage you to take advantage of their expertise when you have questions or need help.\n\n\n3.2.2 Mental Health\nWhat we do in college is not easy, on many fronts. Students are challenged with feelings of depression, anxiety, and self harm at astonishingly high rates. Learning is hard, and you will likely be challenged in college in ways that you haven’t experienced before. Learning while life is hard is even harder. Please take care of yourself. Drink water, eat well, and get more than seven hours of sleep. Have some hobbies, but don’t feel the need to do everything. If you are feeling depressed, withdrawn, anxious, like an impostor, or you are having specific problems with harassment, assault, bias, etc., please seek help. There are many resources on campus, and my door and email are always open.\nW&J’s Student Health and Counseling Center offers confidential counseling services: https://www.washjeff.edu/student-life/student-health-and-counseling-center/\n\n\n3.2.3 Disability Support Services\nWashington & Jefferson College is committed to providing academic accommodations for students with disabilities. This includes individuals with physical disabilities, learning disabilities, and mental health disorders that meet the definition of a disability under the Americans with Disabilities Act. Students who plan to request accommodations should contact the Director for Academic Success as early as possible, although requests may be made at any time. To determine whether you qualify for accommodations, or if you have questions about services and procedures, please call 724-223-6008 or send an email to dss@washjeff.edu.\n\n\n3.2.4 W&J Library\nI highly recommend that you make use of the W&J Library resources while navigating your work this semester. That includes not only primary literature, access to databases, and book references, but also the friendly members of the library staff. They are excellent resources for finding information efficiently, or learning what you can access through W&J subscriptions that might not be freely available on the internet. The W&J Library page for CIS is https://libguides.washjeff.edu/cis.\n\n\n3.2.5 Writing Center\nStaffed by student Peer Writing Tutors, the Writing Center is a free resource available to all W&J students. Writing Tutors from many majors help writers one-on-one in all phases of the writing process, from deciphering assignments, to discussing ideas, to developing an argument, to finalizing a draft. Because proofreading is a last step in that process, you should leave plenty of time (like at least a week) for getting your ideas right before expecting proofreading help. Consultants also can help writers with personal documents, like job and internship applications. The Center welcomes student writers with all varieties of backgrounds and college preparation, including multilingual writers. Please visit the Writing Center’s page (https://mywj.washjeff.edu/office/writing-center) on MyW&J for specific information regarding hours of availability and how to schedule an appointment.\n\n\n3.2.6 Cautions regarding copyright and licensing\nAll documents provided to you (i.e. syllabus, paper prompts, tests, etc…) are the property of the instructor or author. It is a violation of intellectual property to post these online (especially to websites promoting copying/cheating) or to provide them to students not in our class or in future classes. Your papers are your property, and while you can do with them as you wish, it may be a violation of academic integrity to make them available to others who might use them for plagiarism. Basically, keep course materials and your work to yourself except in the process of editing and peer review.\n\n\n3.2.7 Inclusion and Diversity\nThis course is open to anyone interested in data mining and analysis. It is my intent to make all students feel welcome and served in this course by addressing their learning needs. We all (including myself) are continuously learning about different lived experiences and in this course we will encounter anti-racist pedagogy and confront inequities in how data is collected and used. In this course, if you encounter anything said (intentionally or unintentionally) that made you feel uncomfortable, please talk to me about it. If your learning is impacted by your background, or any life event going on outside of class, feel free to talk to me. Even if you choose not to share details, I would be happy to direct you to the right resources on campus.\nThis semester we will address topics which may be emotionally difficult. I acknowledge that each of you has their own specific life history, family context, identity, body—and that these realities have an impact on how you understand and interact with our course materials. Therefore, I ask you to generally familiarize yourself with the content of our discussions ahead of time, and if for any reason you believe you will be unable to participate in a discussion of certain themes or elements, please contact me beforehand and/or seek the support of any of the formal or informal resources available to you on campus, some of which are included in this syllabus. I look forward to creating a safe learning environment together this semester!",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Policies</span>"
    ]
  },
  {
    "objectID": "policies.html#grading-scale",
    "href": "policies.html#grading-scale",
    "title": "3  Course Policies",
    "section": "3.3 Grading Scale",
    "text": "3.3 Grading Scale\n\n\n\nLetter Grade\nPoint Total\n\n\n\n\nA\n93-100\n\n\nA-\n90-92\n\n\nB+\n87-89\n\n\nB\n83-86\n\n\nB-\n80-82\n\n\nC+\n77-79\n\n\nC\n73-76\n\n\nC-\n70-72\n\n\nD+\n67-69\n\n\nD\n63-66\n\n\nD-\n60-62\n\n\nF\n&lt;60",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Policies</span>"
    ]
  },
  {
    "objectID": "groundrules.html",
    "href": "groundrules.html",
    "title": "4  Ground Rules",
    "section": "",
    "text": "These rules are taken almost verbatim from Miriam Posner, and her original version can be found in this tweet.\nTHIS IS NOT A CONTEST. The only way to win is for everyone to win. If anyone has a question during class or outside of it, don’t hesitate to help (while upholding with the Academic Honesty policy).\nYOU ARE NOT BAD AT TECHNOLOGY. That doesn’t even make sense, because technology is not any one thing; it’s a whole bunch of different skills, and it’s really unlikely that you’re bad at all of them.\nYOUR MISTAKE IS ALMOST INVARIABLY MINOR. This is really common when people are learning new coding skills for the first time. I have seen people give up in frustration because they can’t remember where they saved their file. THIS IS FIXABLE. It does not mean you are a hopeless case!\nNO ONE IS AN EXPERT AT EVERYTHING. You just can’t be on the cutting edge of exploratory data analysis, statistical modeling, Python, data visualization, network analysis, GIS, etc., etc. Even seasoned experts are usually only great at one or two of these. Our goal here is to be good generalists!\nWE DO NOT SINK OR SWIM IN MY CLASS. There is no shame in reaching out to me or your classmates for help. Everyone swims or no one swims!",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ground Rules</span>"
    ]
  },
  {
    "objectID": "criteria.html",
    "href": "criteria.html",
    "title": "5  Criteria for Good Reports",
    "section": "",
    "text": "In this course, you’ll write data analysis reports in Jupyter notebooks, combining programming in Python with visualizations and written explanations. You’ll turn in a report for each week’s Workshop assignment, and your tests and final project will also take the form of Jupyter notebook reports.\nGood data analysis requires the combination of clear writing, well-functioning code, and solid quantitative reasoning. Since these reports are a new genre you are learning in this class, you can follow this list of criteria for crafting good reports. I’ll use the same criteria when I assess your work.\n\nFocus and Organization: All questions in the report are answered and all required sections are complete. Report is on time, organized, and easy to follow.\nWriting: Writing is clear, with all visualization and statistical output explained completely, accurately, and in terms of the data.\nCode: Code sections are complete and all code runs without errors. Code is well-commented and easy to read.\nStatistics & Analysis: All analysis is accurate and given in context, both in terms of the data and of the relevant statistical concepts.\nVisualization: Visualizations are clear, accurate, and well-labeled. Good visualizations should be readable on their own but also well-explained in writing.\n\nFor more on what I’m looking for in the writing, visualization, and analysis portions of the reports, you can refer to the How to Explain in CIS241 guide and the ACM guide to describing charts and graphs.",
    "crumbs": [
      "Home",
      "Course Information",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Criteria for Good Reports</span>"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment Breakdown\nOn this page you’ll find prompts for the different assignments in our course, as well as a breakdown of assignments. Additional information about assignments and grading can be found on the Course Policies page.\nSome of these assignments, especially the Weekly Workshops, the Final Project and the Quizzes, will take the form of Jupyter notebook reports. You can refer to the Criteria for Good Reports as a guide to what is expected in these assignments.",
    "crumbs": [
      "Home",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments.html#assignment-breakdown",
    "href": "assignments.html#assignment-breakdown",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nPercentage\n\n\n\n\nWeekly Workshops & Homework\n25%\n\n\nDocumentation Assignment\n10%\n\n\nProject Planning Document\n10%\n\n\nTest 1\n10%\n\n\nTest 2\n10%\n\n\nVideo Presentation\n10%\n\n\nFinal Project\n25%",
    "crumbs": [
      "Home",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/documentation.html",
    "href": "assignments/documentation.html",
    "title": "6  Documentation Assignment",
    "section": "",
    "text": "6.1 Choosing Datasets\nComplete by: Thursday 11 Sept. at class time\nThe goal of this assignment is to get you more familiar with finding, understanding, and documenting a data set. You’ll find three datasets online and create documentation and metadata for one of them, using the guidelines below.\nYou can choose whatever datasets you want so long as they come from reputable sources (see the links below) and they’re large enough for you to write about. These should be rectangular datasets with rows and columns, ideally available as CSV or spreadsheet files. Try to find datasets that have a mix of numerical and categorical variables. You will turn in the three original data files along with your documentation.\nYou can choose data from whatever topics you like. For this project, you’ll find three datasets, each from a different domain or subject. For example, this means that if your first dataset is about baseball, the other two should not be sports datasets.\nRemember that there are data sets out there from almost every area of study that you can imagine! Here are some ideas to get you started:\nDon’t just settle for the very first datasets you find: look around a bit for interesting or unexpected ones!\nLater in the semester, you’ll choose a dataset to work on for your final project. This assignment is unrelated, so you can decide to stick with one of these datasets later on or choose something completely new: it’s entirely up to you.\nMostly, choose data that you are curious about and will allow you to demonstrate the listed concepts for the assignment. Have some fun with it!",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation Assignment</span>"
    ]
  },
  {
    "objectID": "assignments/documentation.html#choosing-datasets",
    "href": "assignments/documentation.html#choosing-datasets",
    "title": "6  Documentation Assignment",
    "section": "",
    "text": "Anthropology and Sociology\nBiology\nEconomics/Business\nPhysics\nPolitical Science\nPsychology\nPhilosophy\nSports Analytics\nArts & Humanities\n\n\n\n\n\n6.1.1 Where can I find datasets?\nThe Data Resources and Repositories page of the DS LibGuide has an extensive list of data resources, including general sites as well as repositories broken down by topic.\nYou may not choose any datasets from Kaggle or Data.world. These are general repositories that often make it more difficult to find the original collectors or purpose of the data.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation Assignment</span>"
    ]
  },
  {
    "objectID": "assignments/documentation.html#writing-your-documentation",
    "href": "assignments/documentation.html#writing-your-documentation",
    "title": "6  Documentation Assignment",
    "section": "6.2 Writing Your Documentation",
    "text": "6.2 Writing Your Documentation\nYou will find three datasets and list them (with links) at the top of the document, and then you’ll choose just one for which you’ll complete full documentation.\nBelow is a suggested format to use to make sure the useful information is communicated to a reader or potential data user (you too!) needed to interpret and understand it. As you edit, remove any instructional or template text and replace with your own. If you have questions particular to your dataset or another idea for how to present the data dictionary and documentation, please discuss in advance with me.\n\n\nName of Dataset 1: Name of collectors, and link to data source. Briefly describe the dataset (rows and columns) and any ethical/logistical considerations.\nName of Dataset 2: Name of collectors, and link to data source. Briefly describe the dataset (rows and columns) and any ethical/logistical considerations.\nThe dataset you chose to fully document:\n\nDataset 3 Name\nData source: Name of data provider, website, and link\nOriginal data collectors: Names of people or organization\nData size: e.g. number of KB or MB\nSpecial permissions: Note if the data is freely available, published, or has restrictions for use.\n1-2 short paragraphs describing the data table(s) you plan to use. Your description should describe what one row (observation) represents, as well as any potentially “tricky” or difficult aspects of the dataset - for example, how is missing data denoted in the dataset? If the dataset already had a well-defined codebook or metadata provided that can give additional information, please link to it here. You can also mention any potential ethical issues one should be aware of when using this data.\n[n.b. You dataset may already have some written documentation when you find it. It’s okay to read that and think about it as you work, but your final paragraphs should all be in your own words.]\nName_of_your_datafile.csv\nData Dictionary:\nPlease note that if your data set has multiple data tables, you will need to provide more than one of these data dictionary tables, one for each dataset. You should make a note about which column in each dataset contains common information, and represents the “link” or the “key” between them.\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nVariable Definition\nData Type\nUnits\nVariable Codes and Ranges\nMissing Value Codes\n\n\n\n\nThe name of the column, exactly as it appears in your .csv file\nDescribe what each variable represents, and if known, how it was measured.\nSay whether the data is numerical or categorical, and give a subcategory (e.g. continuous)\nif units needed; grams, days…\nif not a continuous variable, you could list the acceptable values\nif there is missing data, indicate what is used, e.g. blank, NA, NULL, etc.\n\n\n\nAdd more rows as needed… [n.b. You only need to create a chart of 20 variables. If your data set has more than 20 variables, just choose the 20 that seem the most relevant or interesting.]\nFor Variable Definition, If you don’t know or don’t have information on how to interpret a variable, or want to give a word of caution, say so here. If the variable has any special considerations or challenges inherent to its measure, you may note that here, too.\nFor Variable Codes and Ranges, If you don’t need to specify acceptable values, you can just fill this in with NA (for not applicable).\nData Wrangling:\nWrite a paragraph explaining the wrangling steps you will need to take in order to ensure that this data is in tidy form and ready for analysis.\nThis may include: selecting a subset of the data, creating new columns, renaming existing columns, sorting data, grouping and summarizing, or even splitting columns and pivoting the table. Be sure to ask me if you have questions about what wrangling steps should be taken! You may find it easiest to attempt these wrangling steps yourself, but you do not need to submit the code for this assignment. Be specific about exactly what columns, rows, and values you should change and why.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation Assignment</span>"
    ]
  },
  {
    "objectID": "assignments/documentation.html#requirements",
    "href": "assignments/documentation.html#requirements",
    "title": "6  Documentation Assignment",
    "section": "6.3 Requirements",
    "text": "6.3 Requirements\n\nTurn in your documentation (as a PDF) via Sakai\nFormat the documentation according to the template above\nAlso turn in all 3 original data files that you found online (ideally in CSV format)\nIf you run into any problems with Sakai’s file size limits, you can email the files to me",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation Assignment</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html",
    "href": "assignments/project-planning.html",
    "title": "7  Project Planning Document",
    "section": "",
    "text": "7.1 Description\nComplete by: Thursday 30 Oct. at class time\nYou’ve already learned a lot about how to work with data, and this assignment is designed to get you started thinking about the final project for this class, which will bring all these skills together.\nDon’t worry if you’re still getting the hang of statistical modeling: while that’s part of the final project, exploratory data analysis is also a big important part of the assignment, and you already know all about that!\nAt this stage of the project, you’ll (1) choose a dataset to work on, (2) craft some research questions around that data, (3) conduct some secondary research on your data, (4) consider the ethics of your project, and (5) begin to plan out your analysis. Let’s begin by learning a little more about the final project:",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#description",
    "href": "assignments/project-planning.html#description",
    "title": "7  Project Planning Document",
    "section": "",
    "text": "7.1.1 The Final Project\nThe purpose of the final project is to combine the core skills you have gained in the class to produce a polished report on a question of your choosing, ideally something that you’re passionate about or is relevant to your life or interests.\nThis final project will incorporate all of the steps involved in the Data Analysis Cycle. You will be in charge of stating interesting questions that don’t duplicate previous projects or workshops, conducting exploratory analyses using skills from the entire semester, building models that you will interpret, then communicating your key findings in a polished, professional narrative.\nRemember that the data cycle is an iterative, not linear, process. You may find that your original question must be revised to match the available data or that your data will need restructuring to match the question effectively. Further, your first prediction/explanation models might not be your final models. You’ll revise as you go, and nothing in your plan is set in stone.\nFinally, the final project will be an opportunity to practice your communication with non-technical audiences, as your conclusions should be stated in plain language that could be understood by the general public. You can refer to the full prompt for the Final Project for more.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#choosing-your-data-and-questions",
    "href": "assignments/project-planning.html#choosing-your-data-and-questions",
    "title": "7  Project Planning Document",
    "section": "7.2 Choosing Your Data and Questions",
    "text": "7.2 Choosing Your Data and Questions\nThe first thing you will need is the dataset you will work with and the research questions you will ask about it.\nYour project must analyze a real-world data set, using the course concepts and materials. You should choose a topic and dataset that interests you, and come up with a main question and hypothesis that is answerable. The goal of the project includes demonstrating the breadth of techniques we learned this semester: from data wrangling, visualization, summary stats, hypothesis testing, regression, modeling, and interpretation.\n\n7.2.1 Selecting Data\nYou can choose whatever dataset you like, but I recommend that you choose one of the three datasets that you found at the beginning of the semester, since you’ve been thinking about those for the last several weeks. If you need help finding datasets, refer back to the prompt for the Documentation Assignment.\nConsider that your dataset will need to have certain features in order to conduct certain analyses. For example, if you want to run a regression model, you will need one or more numeric variables. If you want to run a hypothesis test to compare means, you will need a mixture of numeric and categorical variables. Most of our analysis methods work best when you have a fair bit of data, so you probably don’t want a dataset that’s too small. You should choose a dataset while keeping in mind the techniques and methods that we’re learning in this class: this will make it easier for you to choose models and approaches later on.\nThe way to know you’re choosing the right dataset is to have good research questions that will guide you:\n\n\n7.2.2 Asking Questions\nWe can ask questions that are closed (yes or no answer; are the two things different or not?) or open (require more thought and explanation; how much does something change? How is it related to something else?). Likewise, our data analyses can serve one or more purposes as we move through the data analysis cycle: descriptive (describes different measures of the data), exploratory (looking for patterns or unknown relationships in the data); inferential (using a sample to tell us something about a larger population); predictive (use relationships in the current/past data to predict the future); or causal (what happens to one variable when one or more other variables change?).\nYou will want several research questions (a minimum of 3) that help you to address different parts of your analysis, as well as a rationale for how these questions relate to each other.\nMostly, choose something that you are curious about and will allow you to demonstrate the listed concepts for the project. Have some fun with it!",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#sec-secondary",
    "href": "assignments/project-planning.html#sec-secondary",
    "title": "7  Project Planning Document",
    "section": "7.3 Secondary Research",
    "text": "7.3 Secondary Research\nTo help you craft your research questions, you’ll find two secondary sources that are relevant to your data or analysis. These could be existing studies on similar data, articles about particular methods, or summaries of data analysis in your chosen domain. You should include a formal citation (MLA is fine) for each source, briefly describe each one in a sentence or two, and then explain how they relate to your research project.\nYou can use the many resources in the Data Science LibGuide to help you find sources.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#sec-ethics",
    "href": "assignments/project-planning.html#sec-ethics",
    "title": "7  Project Planning Document",
    "section": "7.4 Ethical Considerations",
    "text": "7.4 Ethical Considerations\nConsider some of the ethical challenges that your data presents, and write a paragraph discussing these. Remember: all data projects involve careful consideration of ethics. Address the ethical issues in your project in terms of the ADSA’s four lenses. You’ll be able to include an updated version of this paragraph in your Final Project.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#sec-analysis",
    "href": "assignments/project-planning.html#sec-analysis",
    "title": "7  Project Planning Document",
    "section": "7.5 Planning Your Analysis",
    "text": "7.5 Planning Your Analysis\nYou’ll conclude this assignment with a paragraph discussing how you plan to go about answering your research questions. Among other things, you might discuss:\n\nwhat data wrangling steps you’ll need to take to prepare your data\nwhat visualizations you’ll create and why\nwhat tests and models you might use and how they will help to answer your questions\n\nYour project will naturally change and grow as you work on it: you aren’t stuck with the exact plan you lay out here. But you should show that you’ve thought carefully about what you want to do next. Once you and I have discussed your Project Plan, you’ll be able to get started on the wrangling and exploratory steps!",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/project-planning.html#requirements",
    "href": "assignments/project-planning.html#requirements",
    "title": "7  Project Planning Document",
    "section": "7.6 Requirements",
    "text": "7.6 Requirements\nA PDF report uploaded to Sakai that includes the following:\n\nyour name\ndataset you will use (and hyperlink to cite source)\na list of the research questions you will investigate: you should have at least 3 (see Section 7.2.2)\none paragraph (5-7 sentences) explaining your rationale for the project: why is this dataset and question interesting? How do your research questions relate to one another?\nformal citations for 2 secondary sources, with brief descriptions of each (see Section 7.3)\none paragraph explaining the ethical considerations for your project (see Section 7.4)\none paragraph laying out your analysis plan (see Section 7.5)",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project Planning Document</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html",
    "href": "assignments/presentation.html",
    "title": "8  Presentation",
    "section": "",
    "text": "8.1 Creating a Video\nComplete by: Thursday 25 Nov. at class time\nYou will prepare a 3-5 minute “elevator pitch” video presentation about your project for your classmates, along with a short written summary for me. The goal of this video is not to summarize the entire content of your project, but rather to explain to your audience your chosen data set, the questions you have, and the approaches you plan to take, in a way that will generate interest and allow others to offer suggestions.\nSpecifically, your video should meet the following guidelines:\nAudience: Your audience is the other students in this class (not just me!). Remember that they don’t know anything about your data or your research questions yet. The goal of this presentation is to explain to them what you’re trying to find out, why it matters to you, and how you will answer your questions.\nContent: Your presentation should not attempt to be a summary of your final project report. For one thing, you probably haven’t written the whole thing yet! You also shouldn’t simply read your introduction or your conclusion. This presentation should be based on content that addresses three main questions:\nYour content must reflect the expected knowledge of your audience. This means choosing your words carefully and defining technical terminology (though for this context, informal or intuitive definitions can be acceptable so long as they are not incorrect or misleading).\nFormat: Your video must be at least three minutes and may not be longer than five minutes. Note that this means that you can, and must, practice your presentation to ensure it meets these guidelines. You will make your video available through Microsoft Stream by uploading a video in a format such as an mp4 or other Stream compatible file format. Details about submitting your video are provided below.\nYou will be assessed on the content of this video and how well it addresses the Content requirements laid out above in an Audience aware manner. You will not be assessed on the production quality, so long as the audio and video are clear. Your video may simply be a recording of you talking taken through your webcam or phone camera. You may also provide an audio track over visuals if you prefer. However, your goal is to be compelling to a general student audience in an “elevator pitch” style, so do not narrate over a traditional PowerPoint with titles, bullet points, etc.\nOnce videos are posted, you’ll be expected to watch your classmate’s videos and prepare to discuss them during class time. More information on this below.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#creating-a-video",
    "href": "assignments/presentation.html#creating-a-video",
    "title": "8  Presentation",
    "section": "",
    "text": "What data set are you investigating, why is it important, and what questions about it are you exploring?\nWhat are the one or two most interesting things that you learned while completing this project so far?\nWhat are the next steps you plan to take and why are these the right steps?",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#written-progress-summary",
    "href": "assignments/presentation.html#written-progress-summary",
    "title": "8  Presentation",
    "section": "8.2 Written Progress Summary",
    "text": "8.2 Written Progress Summary\nYou will also produce a 1–2 page written summary that may overlap with some of the content in your video. This is not simply the script of your video—it should be a distinct document. This summary should include:\n\n1 paragraph on where you are currently at in your data analysis\nAt least 2 polished visuals using your dataset, well-labeled and described\n1 paragraph describing what you plan to do in the Statistical Analysis and Interpretation part of your report: what methods might you use, and what will you need to do with your data to complete this section?\n1 paragraph covering your triumphs and challenges:\n\nShare something about your dataset that you have learned so far\nShare something that you have found difficult, confusing, or haven’t figured out yet\n\n\nYou’ll submit this document as a PDF on Sakai. You can copy your visualizations as static images into the word processor of your choice—let me know if you have questions about this.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#how-to-submit-a-video",
    "href": "assignments/presentation.html#how-to-submit-a-video",
    "title": "8  Presentation",
    "section": "8.3 How to Submit a Video",
    "text": "8.3 How to Submit a Video\n\nPlease submit videos to the OneDrive folder for our class (there is a link to the folder on Sakai).\nYou can record the video directly in Microsoft Stream or upload a compatible file to Stream/OneDrive.\nThe video will need to be in a compatible format: .mp4 is the most common, but .flv, .mxf, .gxf, .mpg, .wmv, .asf, .avi. wav, and .mov would also work.\nYou MUST click on your video to make sure it plays properly. You are responsible for ensuring that your video is posted on time and that it works when your classmates attempt to watch it.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#responding-to-videos",
    "href": "assignments/presentation.html#responding-to-videos",
    "title": "8  Presentation",
    "section": "8.4 Responding to Videos",
    "text": "8.4 Responding to Videos\nIn addition to making a video presentation, you will watch and respond to the videos made by your peers. Before class on Thurs. 4 Dec., you will watch all the videos. After watching a video, you will write down the following for each one.\n\nOne substantive similarity between the presenter’s project and your own.\nOne question you have about the project.\nOne idea or suggestion you have for the presenter as they finish their project.\n\nBring these similarities, questions, and suggestions with you to class, and you will use them as the basis to discuss the projects in your groups. You’ll also use these as the start of our Q&A session for the panel presentations.\nAt the end of the class, you can post your three items as a comment attached to your classmate’s video on Stream. I’ll read over each response and will include this as part of your presentation grade.\nRemember: the best comments are ones that are substantive and move the conversation forward. So not just “We are both working on golf data,” but “In my project, I found that one challenge with golf data was that it had mostly quantitative variables. You’re dealing with a similar challenge.” And not just “What do you plan to do next?” but “When you model your data with a random forest classifier, do you plan to limit the size of your decision trees?” These comments can be genuinely helpful feedback for your classmates, so give the feedback that you would like to receive!",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#panel-discussions",
    "href": "assignments/presentation.html#panel-discussions",
    "title": "8  Presentation",
    "section": "8.5 Panel Discussions",
    "text": "8.5 Panel Discussions\nDuring our final exam session, we’ll have panel discussions about your final projects. Your final projects will be due before this session begins, and all of your classmates will have watched your video presentation.\nYou do not need to prepare a separate presentation for this session, and there is no final exam. Each group will take questions from the rest of the class about their videos and their final projects. You can use the responses you already wrote for the videos as questions for the Q&A portion. I will explain more about the format in class. Participation in the panel discussion is part of the presentation grade.\nThe goal of these discussions is to reflect on what you accomplished in your project and compare that to what you had initially planned.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/presentation.html#requirements",
    "href": "assignments/presentation.html#requirements",
    "title": "8  Presentation",
    "section": "8.6 Requirements",
    "text": "8.6 Requirements\n\nYour video presentation, uploaded to the OneDrive folder\nYour written summary, submitted as PDF to the Sakai assignment\nResponses for each video, brought with you to class on Thurs. 4 Dec.\nParticipation in the panel discussions on Dec. 13 at 2pm",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "assignments/final-report.html",
    "href": "assignments/final-report.html",
    "title": "9  Final Project",
    "section": "",
    "text": "9.1 Introduction\nComplete by: Saturday 13 Dec. at 2pm\nPlease note that I cannot accept any work past this deadline.\nA polished Jupyter Notebook HTML file reporting the results of your final data analysis project, as outlined in your Project Planning Document. Roughly, 5-7 written pages (though this is hard to measure in a Jupyter notebook, so consider it a guideline). Think about this report as a “final takeaway” of all the skills you’ve learned in class over the semester. Below is a rough structure of your final written report.\nThis should be a ready-to-deliver report with clear section headers and interpretations of any statistical or graphical output (like several of our previous projects). You can review the Sample Final Project to get a sense of what you’ll need to accomplish.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "assignments/final-report.html#introduction",
    "href": "assignments/final-report.html#introduction",
    "title": "9  Final Project",
    "section": "",
    "text": "Provide a one-to-three paragraph introduction, professionally written, that gives an overview of the essentials someone needs to know to make sense of the data you show.\nConsider some of the ethical and logistical challenges that your data presents, and discuss this in your introduction. Address the ethical issues in your project in terms of the ADSA’s four lenses.\nYou may reuse some of the text you wrote in your Project Planning document, but think carefully about how to revise and expand it.\nYou must cite and link to your dataset, and you can use Markdown to create contextual links like so: [text here](website).",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "assignments/final-report.html#data-explanation-and-exploration",
    "href": "assignments/final-report.html#data-explanation-and-exploration",
    "title": "9  Final Project",
    "section": "9.2 Data Explanation and Exploration",
    "text": "9.2 Data Explanation and Exploration\n\nProvide some details describing the data you are working with. What are the observations? The key variables you will be looking at? Are there any particular challenges in the data you will need to work through or be aware of during analysis?\nPresent at least two univariate or bivariate analyses of key attributes in your data—these may be summary statistics, confidence intervals, distributions, correlations, or other observations. These may be closely related to the visualizations you create in this section.\nProvide four polished visuals that describe the data in a way relevant to your question (descriptive, not related to your statistical model specifically–not a regression plot). No more than two of these should be the same visualization type. Write text that describes the data and what the visuals tell you about your data or decisions you will need to make for the analysis.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "assignments/final-report.html#statistical-analysis-and-interpretation",
    "href": "assignments/final-report.html#statistical-analysis-and-interpretation",
    "title": "9  Final Project",
    "section": "9.3 Statistical Analysis and Interpretation",
    "text": "9.3 Statistical Analysis and Interpretation\n\nProvide at least two distinct statistical approaches (for example: linear regression and hypothesis testing; naive bayes classification and Kmeans clustering; KNN regression and random forest classification; etc.) that you interpret correctly and fully in the text. These can be whatever you choose, but you should explain why you chose the model you did, and why they fit the data. It’s recommended that you use two different approaches (i.e. not just two methods for regression or two methods for classification).\nProvide at least three polished visuals that specifically support and validate the model(s) you have developed (e.g., residual and regression line/scatter, histogram showing normality of data or residuals, confusion matrix, etc.), or help to communicate your main result. Visuals should have captions and be referred to clearly in your text, and they should not all be the same (e.g., not three scatterplots). You’ll likely need more than three to complete the usual validation steps.\nText should fully explain what you show and your findings, to someone who is unfamiliar with your data, code, and models, in terms of the data and in plain language.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "assignments/final-report.html#conclusions",
    "href": "assignments/final-report.html#conclusions",
    "title": "9  Final Project",
    "section": "9.4 Conclusions",
    "text": "9.4 Conclusions\n\nProvide one or two paragraphs concluding about the data: what does it tell us, what are the limitations to this data/model, and what is one future direction you could envision for future data analysts or data collectors?\nInclude citations for the two secondary references that you used in your Project Planning document, and explain how they pertain to your conclusions and insights. You may cite a reference by linking directly to it in your markdown [text here](link here), and listing the full citation below the conclusions section. Please ask me if you aren’t sure how to cite references.",
    "crumbs": [
      "Home",
      "Assignments",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "guides/jupyterhub.html",
    "href": "guides/jupyterhub.html",
    "title": "10  JupyterHub",
    "section": "",
    "text": "10.1 Setting Up JupyterHub\nThis page will help you get set up to use Python and JupyterHub, a data science and programming interface. You will use the Jupyter Notebook file 00_getting_started.ipynb.\nhttps://jrladd.com/CIS241/resources/00_getting_started.ipynb\nOr click this link to download the file.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>JupyterHub</span>"
    ]
  },
  {
    "objectID": "guides/jupyterhub.html#setting-up-jupyterhub",
    "href": "guides/jupyterhub.html#setting-up-jupyterhub",
    "title": "10  JupyterHub",
    "section": "",
    "text": "Navigate to the CIS JupyterHub.\nLog in with your W&J username (the part of your email address before the @). Whatever password you enter for the first time will become your password. KEEP TRACK OF WHAT YOU ENTERED SO THAT YOU DON’T GET LOCKED OUT!\nBookmark this page so you can always get back to JupyterHub.\nThat’s it! You can use this site from any machine, and as long as you save your work, your files will follow you everywhere.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>JupyterHub</span>"
    ]
  },
  {
    "objectID": "guides/jupyterhub.html#managing-files",
    "href": "guides/jupyterhub.html#managing-files",
    "title": "10  JupyterHub",
    "section": "10.2 Managing Files",
    "text": "10.2 Managing Files\n\nOn the left side of the screen you’ll see a list of all the files in your JupyterHub. This won’t show anything yet because you don’t have any files yet.\nTo add a new file, copy this link to our file for today: https://jrladd.com/CIS241/resources/00_getting_started.ipynb. In JupyterHub, select File -&gt; Open from URL at the top of the screen. Paste the URL into the box that appears. The file should now appear in your list! (You could also download the file to your computer and upload it to JupyterHub with the upload button, an upward pointing arrow next to the blue + button.)\nTo create a new blank Notebook file, you can always click the blue + button and select Python 3 under Notebook.\nThe button with a folder and a + on it will let you create a new subfolder. You can then drag files into or out of the various folders. You can organize things however you want, but I recommend developing an organization strategy early that works for you. By the end of the term we’ll have a lot of files! You might create a folder for “workshops” and a folder for “class”, for example.\n\nNow you’re ready to begin! Read through the Getting Started notebook and follow the instructions. You don’t need to code anything yourself today—just follow the examples and get a feel for how Jupyter works.\nn.b. We’re still experimenting with JupyterHub. If you experience any problems with how it works (e.g. it runs really slowly, files go missing, really anything that seems odd), let me know right away. You can always run Jupyter locally on your own laptop or on any of our classroom or lab computers. See this page for instructions on working with Jupyter locally.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>JupyterHub</span>"
    ]
  },
  {
    "objectID": "guides/python.html",
    "href": "guides/python.html",
    "title": "11  Python",
    "section": "",
    "text": "11.1 Variables\nThis first chapter outlines the basic ways to work in Python. If you’re familiar with other programming languages, you’ll likely spot some overlap.\nUse the = sign to store anything inside a variable.\nmyvar = 5\nprint(myvar)\n\n5\nUse descriptive variable names and avoid spaces! You can try:\nVariables have distinct types, and you can find out a variable’s data type with the type() function.\ntype(myvar)\n\nint\nstringvar = \"five\"\ntype(stringvar)\n\nstr",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "guides/python.html#variables",
    "href": "guides/python.html#variables",
    "title": "11  Python",
    "section": "",
    "text": "snake_case\ncamelCase\nOrEven_mix_it_up",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "guides/python.html#comments",
    "href": "guides/python.html#comments",
    "title": "11  Python",
    "section": "11.2 Comments",
    "text": "11.2 Comments\nUse the # sign at the start of a line to create a comment.\n\n# This variable contains a continuous value\nsome_variable = 2.5\n\nYou can use comments to keep track of what you’re using, leave a note for someone working after you, and especially to create a citation if the code you’re using isn’t your own.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "guides/python.html#data-structures",
    "href": "guides/python.html#data-structures",
    "title": "11  Python",
    "section": "11.3 Data Structures",
    "text": "11.3 Data Structures\nPython contains several useful data structures. These overlap with the data structures available in other languages, but they often have different names.\n\n11.3.1 Lists\nLists are analogous to arrays in other languages. They use brackets to contain ordered lists of data.\n\nmylist = [5,6,7]\n\nsecondlist = [\"cat\",\"dog\",\"fish\"]\n\nprint(mylist)\n\n[5, 6, 7]\n\n\nIn a list, you can easily access items using “list slicing,” a bracket notation that lets you access list items with their index.\n\n# Get the first item in a list, at index 0\nmylist[0]\n\n5\n\n\n\n# Get several items in a list, with a range of values\nsecondlist[1:3]\n\n['dog', 'fish']\n\n\n\n# Get items from the end of a list with negative values\nmylist[-1]\n\n7\n\n\nYou can also create a list of numbers (starting at 0) with the range() function:\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython also has tuples, which are a lot like lists but use parentheses instead of brackets: i.e. (1, 2, 3). Tuples often include just 2 items at a time.\n\n\n\n\n11.3.2 Sets\nSets are like lists, but they include only unique items. They use braces instead of brackets.\n\nmyset = {1, 2, 3, 4}\nprint(myset)\n\n{1, 2, 3, 4}\n\n\nYou can turn a list into a set to get only the unique items in that list:\n\nrepeating_list = [1, 2, 1, 3, 3, 5, 4, 6, 4, 1]\nset(repeating_list)\n\n{1, 2, 3, 4, 5, 6}\n\n\nSets have special operators to combine sets and find intersections.\n\nset1 = {1, 2, 3}\nset2 = {3, 4, 5}\n\n# Items in either set\nset1 | set2\n\n{1, 2, 3, 4, 5}\n\n\n\n# Items in both sets\nset1 & set2\n\n{3}\n\n\n\n# Items in one set but not the other\nset1 - set2\n\n{1, 2}\n\n\n\n# Items in either set but not both\nset1 ^ set2\n\n{1, 2, 4, 5}\n\n\n\n\n11.3.3 Dictionaries\nDictionaries are analogous to objects in other languages. They contain key-value pairs, and they’re also surrounded by brackets.\n\nmydictionary = {\"pet_name\": \"Fido\", \"age\": 5, \"pet_type\": \"dog\"}\nprint(mydictionary)\n\n{'pet_name': 'Fido', 'age': 5, 'pet_type': 'dog'}\n\n\nYou can access items in a dictionary using bracket notation, similar to list slicing.\n\nmydictionary[\"pet_name\"]\n\n'Fido'\n\n\n\nmydictionary[\"age\"]\n\n5\n\n\nIf you have two lists or sets of the same length, you can use zip() to combine them as the keys and values of a new dictionary.\n\n\n\n\n\n\nCaution\n\n\n\nThe zip function returns a generator (see below), so you need to convert it to a dictionary with the dict() function.\n\n\n\n# Keep in mind that dictionary keys must be unique values\n\nzipped_dictionary = zip(secondlist, mylist)\ndict(zipped_dictionary)\n\n{'cat': 5, 'dog': 6, 'fish': 7}\n\n\n\n\n11.3.4 Generators/Iterators\nSome functions and methods in Python output generators instead of a distinct data type. These generators often need to be converted to a specific data type before they can be used.\nFor example, the dictionary method .values() gets the values of a dictionary as an iterator:\n\nmydictionary.values()\n\ndict_values(['Fido', 5, 'dog'])\n\n\nWe can see those values above, but we can’t access them in the way we would a list. The code below will throw an error:\n\nmydictionary.values()[1]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 mydictionary.values()[1]\n\nTypeError: 'dict_values' object is not subscriptable\n\n\n\nTo make this work, we need to convert the generator into a list:\n\ndict_values = mydictionary.values()\ndict_values = list(dict_values)\n\ndict_values[1]\n\n5",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "guides/python.html#manipulating-data-structures",
    "href": "guides/python.html#manipulating-data-structures",
    "title": "11  Python",
    "section": "11.4 Manipulating Data Structures",
    "text": "11.4 Manipulating Data Structures\nOnce you know how to create and access data structures, you can use some basic python concepts to create them.\n\n11.4.1 Loops\nThe statements for and in let you loop through a list. This is called, straightforwardly, a “for loop.”\n\n# The variable name after the word `for` is a new name you're creating\nfor x in mylist:\n    print(x)\n\n5\n6\n7\n\n\nYou can do anything you want inside a for loop to manipulate each item:\n\nfor x in mylist:\n    y = x*5\n    print(y)\n\n25\n30\n35\n\n\nIf your goal is to create a new list with some altered items, you can put a for loop into brackets and run all your code on a single line. This is a special Python concept called a “list comprehension.” They keep your code concise and easy to read. The code below does the same thing as above, but puts the items into a brand new list:\n\nnewlist = [x*5 for x in mylist]\nnewlist\n\n[25, 30, 35]\n\n\nFor loops aren’t just for lists! You can use for loops with dictionaries, but you need to use the .items() method:\n\nfor k,v in mydictionary.items():\n    print(k,v)\n\npet_name Fido\nage 5\npet_type dog\n\n\n\n\n11.4.2 Conditions\nCommonly used inside loops, conditions let you evaluate true or false statements. They will perform one action if a statement is true and a different action if the statement is false (else).\nConditions typically use operators to compare things to one another. These operators include:\n\n== is equal to\n!= is not equal to\n&gt; is greater than\n&gt;= is greater than or equal to\n&lt; is less than\n&lt;= is less than or equal to\n\n\nfor x in mylist:\n    if x == 7:\n        print(\"Hooray!\")\n    else:\n        print(\"Hip\")\n\nHip\nHip\nHooray!\n\n\nYou can also use elif to create a chain of multiple conditions.\n\nfor x in [5, 6, 7, 8]:\n    if x/2 == 3 or x/2 == 4:\n        print(x*10)\n    elif x &lt; 6:\n        print(\"small and odd\")\n    else:\n        print(\"This is seven.\")\n\nsmall and odd\n60\nThis is seven.\n80\n\n\n\n\n11.4.3 Functions\nFunctions are simply reusable bits of code. In Python, they’re generally a word followed by parentheses. Inside the parentheses are arguments: bits of data or information that the function needs to work. You’ve used functions already: print(), range(), and type() are all built-in Python functions:\n\ntype(mydictionary)\n\ndict\n\n\nYou can easily create your own functions in Python using the def statement. You give your function a name and some arguments, and then you tell it what to do. The function should also return a value.\n\n# As in a for loop, the `arg` variable is one you're creating on the spot.\ndef myFunction(arg):\n    x = arg*10\n    return x\n\nOnce you create a function, it will be stored in memory for you to use later.\n\nmyFunction(5)\n\n50\n\n\n\nmyFunction(8)\n\n80\n\n\n\n\n\n\n\n\nTip\n\n\n\nMethods are a lot like functions, but they are attached to an object with a period. For example, in mydictionary.values(), .values() is a method that gets the dictionary’s values.\n\n\n\n\n11.4.4 Libraries\nSometimes you will write functions yourself, and often you will use functions that others have written. The Python community is very large, full of people who’ve made helpful code that you can reuse. They’ve packaged this code into libraries. Libraries include sets of methods and functions that you can use. In the rest of these guides, we’ll focus on three large data analysis libraries: pandas, altair, and sklearn.\nTo import a library, you can simply use the import statement. If you like, you can also abbreviate the name of the library using as.\n\nimport pandas as pd\n\nGenerally importing libraries is the first thing you do, at the very top of your code. Once you’ve imported a library, you can use any of its functions.\n\n# More on what this function does in the next chapter!\npd.Series(mylist)\n\n0    5\n1    6\n2    7\ndtype: int64\n\n\nYou can also import just small parts of very large libraries. For example, sklearn has linear regression classes that you can import like so:\n\nfrom sklearn.linear_model import LinearRegression\n\nThis can save time and memory when you only need one or two functions.\n\nThat’s it for Python basics! In the next chapters, we’ll see how to use Python to manipulate tabular data.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html",
    "href": "guides/pandas.html",
    "title": "12  Pandas",
    "section": "",
    "text": "12.1 DataFrames and Series\nPandas is a library for storing and manipulating tabular data, or data stored in rows and columns like a spreadsheet. Pandas is a huge library with many different functions and methods, so what follows is a brief introduction to the most important functions for data management.\nInstead of normal Python lists and dictionaries, Pandas stores data in its own specialized objects. The main one is a DataFrame, which is a lot like a spreadsheet with rows and columns.\nYou can create a DataFrame directly with the DataFrame() class in Pandas, but it’s more likely that you’ll read in a DataFrame from a CSV or spreadsheet file. First you must import the library, and it’s a good idea to import the numpy library as well.\nimport pandas as pd\nimport numpy as np\nNow you can use the read_csv() function to read in a comma-separated value (CSV) spreadsheet file. You must put the name of this file in quotes, and the file should be in the same directory as your Jupyter notebook (or else you should include a full path). The read_csv() function will also accept a URL that points to a CSV file online.\nFor this example, we’ll use the file mpg.csv which comes from R’s ggplot2 library.\nmpg = pd.read_csv(\"../data/mpg.csv\")\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\nYou can get basic information about your DataFrames columns using the .info() method.\nmpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 234 entries, 0 to 233\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   manufacturer  234 non-null    object \n 1   model         234 non-null    object \n 2   displ         234 non-null    float64\n 3   year          234 non-null    int64  \n 4   cyl           234 non-null    int64  \n 5   trans         234 non-null    object \n 6   drv           234 non-null    object \n 7   cty           234 non-null    int64  \n 8   hwy           234 non-null    int64  \n 9   fl            234 non-null    object \n 10  class         234 non-null    object \ndtypes: float64(1), int64(4), object(6)\nmemory usage: 20.2+ KB\nA Series is a lot like a Python list, and each column of a DataFrame is a Series. You can access the columns of a Dataframe with dot notation.\nmpg.model\n\n0          a4\n1          a4\n2          a4\n3          a4\n4          a4\n        ...  \n229    passat\n230    passat\n231    passat\n232    passat\n233    passat\nName: model, Length: 234, dtype: object\nYou can also turn a list into a Series with the Series() class.\nmyseries = pd.Series([5, 6, 7, 8])\nmyseries\n\n0    5\n1    6\n2    7\n3    8\ndtype: int64",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#dataframes-and-series",
    "href": "guides/pandas.html#dataframes-and-series",
    "title": "12  Pandas",
    "section": "",
    "text": "Tip\n\n\n\nNumpy is a Python library for efficiently handling arrays and matrices of numbers. Pandas uses it under the hood to run quickly. You usually won’t need to use it directly, but it’s good to have it installed to avoid any mysterious errors.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nJupyter nicely formats DataFrames as tables when you type the name of a variable containing a DataFrame. But if you use the print() function, it won’t display as well.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#selecting-rows-and-columns",
    "href": "guides/pandas.html#selecting-rows-and-columns",
    "title": "12  Pandas",
    "section": "12.2 Selecting Rows and Columns",
    "text": "12.2 Selecting Rows and Columns\nOnce you have a DataFrame, you’ll typically want to filter and select different rows or columns.\nTo filter specific rows, Pandas uses a bracket notation. It takes conditional statements that are similar to Python conditions.\n\n# Get cars with fewer than 6 cylinders\nfour_cylinders = mpg[mpg.cyl &lt; 6]\nfour_cylinders\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n7\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n226\nvolkswagen\nnew beetle\n2.5\n2008\n5\nauto(s6)\nf\n20\n29\nr\nsubcompact\n\n\n227\nvolkswagen\npassat\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\nmidsize\n\n\n228\nvolkswagen\npassat\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\nmidsize\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n\n\n85 rows × 11 columns\n\n\n\nYou can also use the operators & (and), | (or), and ! (not) to combine conditional filters.\n\n# Get Volkswagens and Fords\nvw_ford = mpg[(mpg.manufacturer == 'volkswagen') | (mpg.manufacturer == 'ford')]\nvw_ford\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n74\nford\nexpedition 2wd\n4.6\n1999\n8\nauto(l4)\nr\n11\n17\nr\nsuv\n\n\n75\nford\nexpedition 2wd\n5.4\n1999\n8\nauto(l4)\nr\n11\n17\nr\nsuv\n\n\n76\nford\nexpedition 2wd\n5.4\n2008\n8\nauto(l6)\nr\n12\n18\nr\nsuv\n\n\n77\nford\nexplorer 4wd\n4.0\n1999\n6\nauto(l5)\n4\n14\n17\nr\nsuv\n\n\n78\nford\nexplorer 4wd\n4.0\n1999\n6\nmanual(m5)\n4\n15\n19\nr\nsuv\n\n\n79\nford\nexplorer 4wd\n4.0\n1999\n6\nauto(l5)\n4\n14\n17\nr\nsuv\n\n\n80\nford\nexplorer 4wd\n4.0\n2008\n6\nauto(l5)\n4\n13\n19\nr\nsuv\n\n\n81\nford\nexplorer 4wd\n4.6\n2008\n8\nauto(l6)\n4\n13\n19\nr\nsuv\n\n\n82\nford\nexplorer 4wd\n5.0\n1999\n8\nauto(l4)\n4\n13\n17\nr\nsuv\n\n\n83\nford\nf150 pickup 4wd\n4.2\n1999\n6\nauto(l4)\n4\n14\n17\nr\npickup\n\n\n84\nford\nf150 pickup 4wd\n4.2\n1999\n6\nmanual(m5)\n4\n14\n17\nr\npickup\n\n\n85\nford\nf150 pickup 4wd\n4.6\n1999\n8\nmanual(m5)\n4\n13\n16\nr\npickup\n\n\n86\nford\nf150 pickup 4wd\n4.6\n1999\n8\nauto(l4)\n4\n13\n16\nr\npickup\n\n\n87\nford\nf150 pickup 4wd\n4.6\n2008\n8\nauto(l4)\n4\n13\n17\nr\npickup\n\n\n88\nford\nf150 pickup 4wd\n5.4\n1999\n8\nauto(l4)\n4\n11\n15\nr\npickup\n\n\n89\nford\nf150 pickup 4wd\n5.4\n2008\n8\nauto(l4)\n4\n13\n17\nr\npickup\n\n\n90\nford\nmustang\n3.8\n1999\n6\nmanual(m5)\nr\n18\n26\nr\nsubcompact\n\n\n91\nford\nmustang\n3.8\n1999\n6\nauto(l4)\nr\n18\n25\nr\nsubcompact\n\n\n92\nford\nmustang\n4.0\n2008\n6\nmanual(m5)\nr\n17\n26\nr\nsubcompact\n\n\n93\nford\nmustang\n4.0\n2008\n6\nauto(l5)\nr\n16\n24\nr\nsubcompact\n\n\n94\nford\nmustang\n4.6\n1999\n8\nauto(l4)\nr\n15\n21\nr\nsubcompact\n\n\n95\nford\nmustang\n4.6\n1999\n8\nmanual(m5)\nr\n15\n22\nr\nsubcompact\n\n\n96\nford\nmustang\n4.6\n2008\n8\nmanual(m5)\nr\n15\n23\nr\nsubcompact\n\n\n97\nford\nmustang\n4.6\n2008\n8\nauto(l5)\nr\n15\n22\nr\nsubcompact\n\n\n98\nford\nmustang\n5.4\n2008\n8\nmanual(m6)\nr\n14\n20\np\nsubcompact\n\n\n207\nvolkswagen\ngti\n2.0\n1999\n4\nmanual(m5)\nf\n21\n29\nr\ncompact\n\n\n208\nvolkswagen\ngti\n2.0\n1999\n4\nauto(l4)\nf\n19\n26\nr\ncompact\n\n\n209\nvolkswagen\ngti\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\ncompact\n\n\n210\nvolkswagen\ngti\n2.0\n2008\n4\nauto(s6)\nf\n22\n29\np\ncompact\n\n\n211\nvolkswagen\ngti\n2.8\n1999\n6\nmanual(m5)\nf\n17\n24\nr\ncompact\n\n\n212\nvolkswagen\njetta\n1.9\n1999\n4\nmanual(m5)\nf\n33\n44\nd\ncompact\n\n\n213\nvolkswagen\njetta\n2.0\n1999\n4\nmanual(m5)\nf\n21\n29\nr\ncompact\n\n\n214\nvolkswagen\njetta\n2.0\n1999\n4\nauto(l4)\nf\n19\n26\nr\ncompact\n\n\n215\nvolkswagen\njetta\n2.0\n2008\n4\nauto(s6)\nf\n22\n29\np\ncompact\n\n\n216\nvolkswagen\njetta\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\ncompact\n\n\n217\nvolkswagen\njetta\n2.5\n2008\n5\nauto(s6)\nf\n21\n29\nr\ncompact\n\n\n218\nvolkswagen\njetta\n2.5\n2008\n5\nmanual(m5)\nf\n21\n29\nr\ncompact\n\n\n219\nvolkswagen\njetta\n2.8\n1999\n6\nauto(l4)\nf\n16\n23\nr\ncompact\n\n\n220\nvolkswagen\njetta\n2.8\n1999\n6\nmanual(m5)\nf\n17\n24\nr\ncompact\n\n\n221\nvolkswagen\nnew beetle\n1.9\n1999\n4\nmanual(m5)\nf\n35\n44\nd\nsubcompact\n\n\n222\nvolkswagen\nnew beetle\n1.9\n1999\n4\nauto(l4)\nf\n29\n41\nd\nsubcompact\n\n\n223\nvolkswagen\nnew beetle\n2.0\n1999\n4\nmanual(m5)\nf\n21\n29\nr\nsubcompact\n\n\n224\nvolkswagen\nnew beetle\n2.0\n1999\n4\nauto(l4)\nf\n19\n26\nr\nsubcompact\n\n\n225\nvolkswagen\nnew beetle\n2.5\n2008\n5\nmanual(m5)\nf\n20\n28\nr\nsubcompact\n\n\n226\nvolkswagen\nnew beetle\n2.5\n2008\n5\nauto(s6)\nf\n20\n29\nr\nsubcompact\n\n\n227\nvolkswagen\npassat\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\nmidsize\n\n\n228\nvolkswagen\npassat\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\nmidsize\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n\n\n\nYou can use a double bracket notation to select a subset of columns.\n\n\n\n\n\n\nTip\n\n\n\nUsing single brackets or dot notation will get you a single column as a Series.\n\n\n\nclass_cty_hwy = mpg[[\"class\", \"cty\", \"hwy\"]]\nclass_cty_hwy\n\n\n\n\n\n\n\n\nclass\ncty\nhwy\n\n\n\n\n0\ncompact\n18\n29\n\n\n1\ncompact\n21\n29\n\n\n2\ncompact\n20\n31\n\n\n3\ncompact\n21\n30\n\n\n4\ncompact\n16\n26\n\n\n...\n...\n...\n...\n\n\n229\nmidsize\n19\n28\n\n\n230\nmidsize\n21\n29\n\n\n231\nmidsize\n16\n26\n\n\n232\nmidsize\n18\n26\n\n\n233\nmidsize\n17\n26\n\n\n\n\n234 rows × 3 columns",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#data-wrangling",
    "href": "guides/pandas.html#data-wrangling",
    "title": "12  Pandas",
    "section": "12.3 Data Wrangling",
    "text": "12.3 Data Wrangling\nIn addtion to selecting rows and columns from DataFrames, you can also use Pandas to do a wide variety of data transformations.\n\n12.3.1 Sorting\n\nmpg.sort_values(\"year\", ascending=False)\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n117\nhyundai\ntiburon\n2.0\n2008\n4\nmanual(m5)\nf\n20\n28\nr\nsubcompact\n\n\n120\nhyundai\ntiburon\n2.7\n2008\n6\nmanual(m6)\nf\n16\n24\nr\nsubcompact\n\n\n122\njeep\ngrand cherokee 4wd\n3.0\n2008\n6\nauto(l5)\n4\n17\n22\nd\nsuv\n\n\n123\njeep\ngrand cherokee 4wd\n3.7\n2008\n6\nauto(l5)\n4\n15\n19\nr\nsuv\n\n\n126\njeep\ngrand cherokee 4wd\n4.7\n2008\n8\nauto(l5)\n4\n9\n12\ne\nsuv\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n130\nland rover\nrange rover\n4.0\n1999\n8\nauto(l4)\n4\n11\n15\np\nsuv\n\n\n50\ndodge\ndakota pickup 4wd\n3.9\n1999\n6\nauto(l4)\n4\n13\n17\nr\npickup\n\n\n51\ndodge\ndakota pickup 4wd\n3.9\n1999\n6\nmanual(m5)\n4\n14\n17\nr\npickup\n\n\n125\njeep\ngrand cherokee 4wd\n4.7\n1999\n8\nauto(l4)\n4\n14\n17\nr\nsuv\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n\n\n234 rows × 11 columns\n\n\n\n\n\n12.3.2 Counting\n\nmpg.value_counts(\"manufacturer\")\n\nmanufacturer\ndodge         37\ntoyota        34\nvolkswagen    27\nford          25\nchevrolet     19\naudi          18\nhyundai       14\nsubaru        14\nnissan        13\nhonda          9\njeep           8\npontiac        5\nland rover     4\nmercury        4\nlincoln        3\nName: count, dtype: int64\n\n\n\n\n12.3.3 Renaming Columns\n\n# Note the use of a Python dictionary as this method's argument\nmpg = mpg.rename({\"cty\":\"city\", \"hwy\": \"highway\"})\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\n\n\n\n\n\n12.3.4 Create new columns\nYou can use assign() to create new columns based on existing ones.\n\nmpg = mpg.assign(displ_per_cyl = mpg.displ/mpg.cyl)\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n0.450000\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n0.450000\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n0.500000\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n0.500000\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n0.466667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n0.500000\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n0.500000\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n0.466667\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n0.466667\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n0.600000\n\n\n\n\n234 rows × 12 columns\n\n\n\n\n\n12.3.5 Grouping and Summarizing\nThis combines a couple functions that exist within Pandas to create summary tables.\nPandas has a wide range of summary statistics that you can apply to individual columns.\n\n# Average city fuel efficiency\nmpg.cty.mean()\n\nnp.float64(16.858974358974358)\n\n\n\n# Standard deviation of highway fuel efficiency\nmpg.hwy.std()\n\nnp.float64(5.9546434411664455)\n\n\nPandas also has a .groupby() method (which returns a generator) that groups categorical variables.\n\nmpg.groupby(\"manufacturer\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x108e9fc50&gt;\n\n\nBy itself, .groupby() doesn’t show anything. It needs to be combined with a summary statistic to create a summary table.\n\n# Averages by manufacturer\n# set `numeric_only=True` to avoid a warning\nmpg.groupby(\"manufacturer\").mean(numeric_only=True)\n\n\n\n\n\n\n\n\ndispl\nyear\ncyl\ncty\nhwy\ndispl_per_cyl\n\n\nmanufacturer\n\n\n\n\n\n\n\n\n\n\naudi\n2.544444\n2003.500000\n5.222222\n17.611111\n26.444444\n0.484722\n\n\nchevrolet\n5.063158\n2004.684211\n7.263158\n15.000000\n21.894737\n0.686842\n\n\ndodge\n4.378378\n2004.108108\n7.081081\n13.135135\n17.945946\n0.616216\n\n\nford\n4.536000\n2002.600000\n7.200000\n14.000000\n19.360000\n0.633667\n\n\nhonda\n1.711111\n2003.000000\n4.000000\n24.444444\n32.555556\n0.427778\n\n\nhyundai\n2.428571\n2004.142857\n4.857143\n18.642857\n26.857143\n0.509524\n\n\njeep\n4.575000\n2005.750000\n7.250000\n13.500000\n17.625000\n0.627604\n\n\nland rover\n4.300000\n2003.500000\n8.000000\n11.500000\n16.500000\n0.537500\n\n\nlincoln\n5.400000\n2002.000000\n8.000000\n11.333333\n17.000000\n0.675000\n\n\nmercury\n4.400000\n2003.500000\n7.000000\n13.250000\n18.000000\n0.633333\n\n\nnissan\n3.269231\n2003.846154\n5.538462\n18.076923\n24.615385\n0.589744\n\n\npontiac\n3.960000\n2002.600000\n6.400000\n17.000000\n26.400000\n0.615833\n\n\nsubaru\n2.457143\n2004.142857\n4.000000\n19.285714\n25.571429\n0.614286\n\n\ntoyota\n2.952941\n2002.705882\n5.117647\n18.529412\n24.911765\n0.573897\n\n\nvolkswagen\n2.255556\n2002.666667\n4.592593\n20.925926\n29.222222\n0.491049\n\n\n\n\n\n\n\n\n\n12.3.6 Dropping Null Values\nFor many statistical modeling tasks, you need to drop rows that contain null values. Pandas lets you do this easily with .dropna().\n\n\n\n\n\n\nNote\n\n\n\nPandas typically stores null values as NaN, which stands for “not a number.”\n\n\n\n# Drop any row that contains a null value in any column\nmpg = mpg.dropna()\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n0.450000\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n0.450000\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n0.500000\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n0.500000\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n0.466667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n0.500000\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n0.500000\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n0.466667\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n0.466667\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n0.600000\n\n\n\n\n234 rows × 12 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe mpg dataset doesn’t contain any null values, so right now this code isn’t accomplishing anything. But it will work when null values are present!\n\n\nYou can also drop null values from only a subset of columns.\n\n# Drop any rows that contain null values in a subset of columns\nmpg = mpg.dropna(subset=[\"model\", \"displ\"])\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n0.450000\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n0.450000\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n0.500000\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n0.500000\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n0.466667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n0.500000\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n0.500000\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n0.466667\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n0.466667\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n0.600000\n\n\n\n\n234 rows × 12 columns",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#sampling",
    "href": "guides/pandas.html#sampling",
    "title": "12  Pandas",
    "section": "12.4 Sampling",
    "text": "12.4 Sampling\nMany statistical methods, especially hypothesis tests, require you to take a random sample of your overall data. Again, Pandas provides an easy way to do this with the .sample() method.\nYou can take a sample of rows from an entire dataframe.\n\n# Get 5 random rows from mpg\nmpg.sample(5)\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n109\nhyundai\nsonata\n2.4\n1999\n4\nmanual(m5)\nf\n18\n27\nr\nmidsize\n0.600000\n\n\n71\ndodge\nram 1500 pickup 4wd\n5.2\n1999\n8\nmanual(m5)\n4\n11\n16\nr\npickup\n0.650000\n\n\n152\nnissan\npathfinder 4wd\n4.0\n2008\n6\nauto(l5)\n4\n14\n20\np\nsuv\n0.666667\n\n\n123\njeep\ngrand cherokee 4wd\n3.7\n2008\n6\nauto(l5)\n4\n15\n19\nr\nsuv\n0.616667\n\n\n43\ndodge\ncaravan 2wd\n3.3\n2008\n6\nauto(l4)\nf\n11\n17\ne\nminivan\n0.550000\n\n\n\n\n\n\n\nYou can also get a sample of a specific column.\n\n# Get 5 sample engine displacement values, as a series\nmpg.displ.sample(5)\n\n162    2.5\n215    2.0\n160    2.5\n125    4.7\n48     3.7\nName: displ, dtype: float64\n\n\nYou can also sample with replacement. (This is also called “bootstrap sampling.”) This makes it possible to have the same value in your sample twice.\n\nmpg.displ.sample(5, replace=True)\n\n157    3.8\n71     5.2\n158    5.3\n162    2.5\n211    2.8\nName: displ, dtype: float64\n\n\nPandas will also let you get a fraction of values instead of a set number in your sample.\n\n# Get a random sample of one twentieth the size of the dataset\nmpg.sample(frac=.05)\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n21\nchevrolet\nc1500 suburban 2wd\n5.7\n1999\n8\nauto(l4)\nr\n13\n17\nr\nsuv\n0.712500\n\n\n22\nchevrolet\nc1500 suburban 2wd\n6.0\n2008\n8\nauto(l4)\nr\n12\n17\nr\nsuv\n0.750000\n\n\n192\ntoyota\ncamry solara\n3.3\n2008\n6\nauto(s5)\nf\n18\n27\nr\ncompact\n0.550000\n\n\n132\nland rover\nrange rover\n4.4\n2008\n8\nauto(s6)\n4\n12\n18\nr\nsuv\n0.550000\n\n\n38\ndodge\ncaravan 2wd\n3.0\n1999\n6\nauto(l4)\nf\n17\n24\nr\nminivan\n0.500000\n\n\n194\ntoyota\ncorolla\n1.8\n1999\n4\nauto(l4)\nf\n24\n33\nr\ncompact\n0.450000\n\n\n20\nchevrolet\nc1500 suburban 2wd\n5.3\n2008\n8\nauto(l4)\nr\n14\n20\nr\nsuv\n0.662500\n\n\n175\ntoyota\n4runner 4wd\n3.4\n1999\n6\nauto(l4)\n4\n15\n19\nr\nsuv\n0.566667\n\n\n198\ntoyota\nland cruiser wagon 4wd\n4.7\n1999\n8\nauto(l4)\n4\n11\n15\nr\nsuv\n0.587500\n\n\n163\nsubaru\nforester awd\n2.5\n2008\n4\nauto(l4)\n4\n20\n26\nr\nsuv\n0.625000\n\n\n216\nvolkswagen\njetta\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\ncompact\n0.500000\n\n\n149\nnissan\nmaxima\n3.5\n2008\n6\nauto(av)\nf\n19\n25\np\nmidsize\n0.583333\n\n\n\n\n\n\n\nThere’s one more trick you can do with sampling. Sometimes you don’t need to get a smaller random sample: instead, you just want to reshuffle every row of the dataset. You can do this by setting frac to 1. In a way, you’re taking a random sample that is 100% of the size of the dataset! (But make sure you do this without replacement.)\n\nmpg.displ.sample(frac=1)\n\n48     3.7\n17     4.2\n24     5.7\n173    2.7\n55     5.2\n      ... \n73     5.9\n10     2.0\n81     4.6\n172    2.5\n32     2.4\nName: displ, Length: 234, dtype: float64\n\n\nPandas will remember the indices in your new Series, which means if you use this reordered sample it might put things back in order for you! To avoid this, you can reset the index and drop the old labels.\n\nmpg.displ.sample(frac=1).reset_index(drop=True)\n\n0      3.5\n1      5.3\n2      1.8\n3      2.5\n4      5.7\n      ... \n229    4.7\n230    2.0\n231    2.0\n232    2.0\n233    2.4\nName: displ, Length: 234, dtype: float64",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#combining-datasets",
    "href": "guides/pandas.html#combining-datasets",
    "title": "12  Pandas",
    "section": "12.5 Combining Datasets",
    "text": "12.5 Combining Datasets\n\n12.5.1 Stack Data with Concatenation\nIf you have two datasets with identical columns but different rows, you can combine or “stack” the two datasets into one.\n\n# Let's imagine we have two datasets.\n# One for manual transmission cars:\n\nmanual = pd.read_csv(\"../data/manual.csv\")\nmanual\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n1\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n2\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\n3\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\n4\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n72\nvolkswagen\nnew beetle\n2.0\n1999\n4\nmanual(m5)\nf\n21\n29\nr\nsubcompact\n\n\n73\nvolkswagen\nnew beetle\n2.5\n2008\n5\nmanual(m5)\nf\n20\n28\nr\nsubcompact\n\n\n74\nvolkswagen\npassat\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\nmidsize\n\n\n75\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n76\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n\n\n77 rows × 11 columns\n\n\n\n\n# And one for automatic transmissions:\nautomatic = pd.read_csv(\"../data/automatic.csv\")\nautomatic\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n2\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n3\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\n4\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n152\nvolkswagen\nnew beetle\n2.5\n2008\n5\nauto(s6)\nf\n20\n29\nr\nsubcompact\n\n\n153\nvolkswagen\npassat\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\nmidsize\n\n\n154\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n155\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n156\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n157 rows × 11 columns\n\n\n\n\n# You can combine these into one with pd.concat()\n\nmpg_concat = pd.concat([manual, automatic])\nmpg_concat\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n1\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n2\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\n3\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\n4\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n152\nvolkswagen\nnew beetle\n2.5\n2008\n5\nauto(s6)\nf\n20\n29\nr\nsubcompact\n\n\n153\nvolkswagen\npassat\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\nmidsize\n\n\n154\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n155\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n156\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\n\n\n\n\n\n12.5.2 Switch Rows and Columns with Pivot and Melt\nSometimes your data isn’t tidy, and rows should be columns while columns should be rows. You can resolve these problems using Pandas’ “pivot” and “melt” concepts—this is similar to Pivot Tables in Excel.\n\n# Let's imagine you have a dataset with column headings in the rows\n\nlong_mpg = pd.read_csv(\"../data/melted_mpg.csv\")\nlong_mpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\nyear\ntrans\nvariable\nvalue\n\n\n\n\n0\naudi\na4\n1999\nauto(l5)\ndispl\n1.8\n\n\n1\naudi\na4\n1999\nmanual(m5)\ndispl\n1.8\n\n\n2\naudi\na4\n2008\nmanual(m6)\ndispl\n2.0\n\n\n3\naudi\na4\n2008\nauto(av)\ndispl\n2.0\n\n\n4\naudi\na4 quattro\n1999\nmanual(m5)\ndispl\n1.8\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n968\nvolkswagen\nnew beetle\n2008\nauto(s6)\nclass\nsubcompact\n\n\n969\nvolkswagen\npassat\n1999\nmanual(m5)\nclass\nmidsize\n\n\n970\nvolkswagen\npassat\n1999\nauto(l5)\nclass\nmidsize\n\n\n971\nvolkswagen\npassat\n2008\nauto(s6)\nclass\nmidsize\n\n\n972\nvolkswagen\npassat\n2008\nmanual(m6)\nclass\nmidsize\n\n\n\n\n973 rows × 6 columns\n\n\n\n\n# Use pivot to turn the \"variable\" column into the column names\n# You must specify which columns you want to retain as indices\n# Make sure you add .reset_index()\n\npivoted_mpg = long_mpg.pivot(index=[\"manufacturer\", \"model\", \"year\", \"trans\"], columns=\"variable\", values=\"value\").reset_index()\npivoted_mpg\n\n\n\n\n\n\n\nvariable\nmanufacturer\nmodel\nyear\ntrans\nclass\ncty\ncyl\ndispl\ndrv\nfl\nhwy\n\n\n\n\n0\naudi\na4\n1999\nauto(l5)\ncompact\n18\n4\n1.8\nf\np\n29\n\n\n1\naudi\na4\n1999\nmanual(m5)\ncompact\n21\n4\n1.8\nf\np\n29\n\n\n2\naudi\na4\n2008\nauto(av)\ncompact\n21\n4\n2.0\nf\np\n30\n\n\n3\naudi\na4\n2008\nmanual(m6)\ncompact\n20\n4\n2.0\nf\np\n31\n\n\n4\naudi\na4 quattro\n1999\nauto(l5)\ncompact\n16\n4\n1.8\n4\np\n25\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\nvolkswagen\nnew beetle\n2008\nmanual(m5)\nsubcompact\n20\n5\n2.5\nf\nr\n28\n\n\n135\nvolkswagen\npassat\n1999\nauto(l5)\nmidsize\n18\n4\n1.8\nf\np\n29\n\n\n136\nvolkswagen\npassat\n1999\nmanual(m5)\nmidsize\n21\n4\n1.8\nf\np\n29\n\n\n137\nvolkswagen\npassat\n2008\nauto(s6)\nmidsize\n19\n4\n2.0\nf\np\n28\n\n\n138\nvolkswagen\npassat\n2008\nmanual(m6)\nmidsize\n21\n4\n2.0\nf\np\n29\n\n\n\n\n139 rows × 11 columns\n\n\n\n\n# You can use .melt() to go in the opposite direction\n# Again you must specify the columns you want to keep as indices\n\nmelted_mpg = pivoted_mpg.melt(id_vars = [\"manufacturer\", \"model\", \"year\", \"trans\"])\nmelted_mpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\nyear\ntrans\nvariable\nvalue\n\n\n\n\n0\naudi\na4\n1999\nauto(l5)\nclass\ncompact\n\n\n1\naudi\na4\n1999\nmanual(m5)\nclass\ncompact\n\n\n2\naudi\na4\n2008\nauto(av)\nclass\ncompact\n\n\n3\naudi\na4\n2008\nmanual(m6)\nclass\ncompact\n\n\n4\naudi\na4 quattro\n1999\nauto(l5)\nclass\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n968\nvolkswagen\nnew beetle\n2008\nmanual(m5)\nhwy\n28\n\n\n969\nvolkswagen\npassat\n1999\nauto(l5)\nhwy\n29\n\n\n970\nvolkswagen\npassat\n1999\nmanual(m5)\nhwy\n29\n\n\n971\nvolkswagen\npassat\n2008\nauto(s6)\nhwy\n28\n\n\n972\nvolkswagen\npassat\n2008\nmanual(m6)\nhwy\n29\n\n\n\n\n973 rows × 6 columns\n\n\n\n\n\n12.5.3 Join Datasets with Merge\nIf your columns or rows don’t match exactly, you can’t use pd.concat(). But you can still combine datasets with a database-style JOIN (just like in SQL!) using the pd.merge() function.\n\n\n\n\n\n\nWarning\n\n\n\nThis database-style combining of tables is really powerful, and it can get more complicated than what we have time to cover in this class. I recommend checking out McKinney Ch. 8.2 for more details, and this all is covered in much more detail in CIS 112!\n\n\n\n# Let's assume you have a dataset without the vehicle class column\n\nmissing_mpg = pd.read_csv(\"../data/mpg_no_class.csv\")\nmissing_mpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\n\n\n\n\n234 rows × 10 columns\n\n\n\n\n# And you also have a dataset (with fewer rows) that has\n# class info for each make and year of vehicle\n\nclass_mpg = pd.read_csv(\"../data/mpg_class.csv\")\nclass_mpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\nyear\nclass\n\n\n\n\n0\naudi\na4\n1999\ncompact\n\n\n1\naudi\na4\n2008\ncompact\n\n\n2\naudi\na4 quattro\n1999\ncompact\n\n\n3\naudi\na4 quattro\n2008\ncompact\n\n\n4\naudi\na6 quattro\n1999\nmidsize\n\n\n...\n...\n...\n...\n...\n\n\n71\nvolkswagen\njetta\n2008\ncompact\n\n\n72\nvolkswagen\nnew beetle\n1999\nsubcompact\n\n\n73\nvolkswagen\nnew beetle\n2008\nsubcompact\n\n\n74\nvolkswagen\npassat\n1999\nmidsize\n\n\n75\nvolkswagen\npassat\n2008\nmidsize\n\n\n\n\n76 rows × 4 columns\n\n\n\n\n# You can add the class column to your dataframe using merge\n\nmerged_mpg = pd.merge(missing_mpg, class_mpg, how=\"left\", on=[\"manufacturer\", \"model\", \"year\"])\nmerged_mpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWes McKinney’s excellent book Python for Data Analysis has lots more examples and many additional function. In particular, you can check out Chapter 5 and Chapter 8.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/pandas.html#data-types",
    "href": "guides/pandas.html#data-types",
    "title": "12  Pandas",
    "section": "12.6 Data Types",
    "text": "12.6 Data Types\nPython and Pandas accommodate many different types of data, and Pandas in particular deals with a range of data types beyond what we’ve already discussed. Usually, you’re likely encounter: numerical data (integers and floats), categorical data (categories and objects), text data (objects and strings), time data (dates and times), and location data (geocoordinates).\n\n\n\n\n\n\nCaution\n\n\n\nRemember, the data types that programming languages use are not always exactly equivalent to the general data types that data scientists might recognize! You’re always trying to use a technical data type that’s the best match for the data you’re trying to store.\n\n\n\n12.6.1 Changing Data Types\nA good way to check what data types are in your data is the .info() method.\n\nmpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 234 entries, 0 to 233\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   manufacturer   234 non-null    object \n 1   model          234 non-null    object \n 2   displ          234 non-null    float64\n 3   year           234 non-null    int64  \n 4   cyl            234 non-null    int64  \n 5   trans          234 non-null    object \n 6   drv            234 non-null    object \n 7   cty            234 non-null    int64  \n 8   hwy            234 non-null    int64  \n 9   fl             234 non-null    object \n 10  class          234 non-null    object \n 11  displ_per_cyl  234 non-null    float64\ndtypes: float64(2), int64(4), object(6)\nmemory usage: 23.8+ KB\n\n\nPandas tries its best to guess the correct data type when your data is being imported. It doesn’t always get things right! For example, if there’s any non-numeric character in a numeric column (like a dollar sign), it will identify that as an object rather than an int or float.\n\n\n\n\n\n\nTip\n\n\n\nPandas will use object for any non-numeric data it doesn’t know how to deal with. Usually, it’s fine to leave this alone for categorical data, but sometimes you will need to use the more robust Category data types that Pandas provides. This lets you label categories and deal with any numeric (ordinal) data that’s actually representing a category.\n\n\nIf a data type is encoded incorrectly, there are two ways to change it:\n\n# The first way is to use the general astype function:\n\nmpg = mpg.assign(manufacturer = mpg.manufacturer.astype(\"category\"))\nmpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 234 entries, 0 to 233\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   manufacturer   234 non-null    category\n 1   model          234 non-null    object  \n 2   displ          234 non-null    float64 \n 3   year           234 non-null    int64   \n 4   cyl            234 non-null    int64   \n 5   trans          234 non-null    object  \n 6   drv            234 non-null    object  \n 7   cty            234 non-null    int64   \n 8   hwy            234 non-null    int64   \n 9   fl             234 non-null    object  \n 10  class          234 non-null    object  \n 11  displ_per_cyl  234 non-null    float64 \ndtypes: category(1), float64(2), int64(4), object(5)\nmemory usage: 22.8+ KB\n\n\n\n# The second way is to use a function specific to\n# convert something to a number, a date, or something else:\n\nmpg = mpg.assign(hwy = pd.to_numeric(mpg.hwy, downcast=\"float\"))\n# The downcast='float' changes your data from an int to a float\n# but you could also use this to go from an object to an int or float\n\nmpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 234 entries, 0 to 233\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   manufacturer   234 non-null    category\n 1   model          234 non-null    object  \n 2   displ          234 non-null    float64 \n 3   year           234 non-null    int64   \n 4   cyl            234 non-null    int64   \n 5   trans          234 non-null    object  \n 6   drv            234 non-null    object  \n 7   cty            234 non-null    int64   \n 8   hwy            234 non-null    float32 \n 9   fl             234 non-null    object  \n 10  class          234 non-null    object  \n 11  displ_per_cyl  234 non-null    float64 \ndtypes: category(1), float32(1), float64(2), int64(3), object(5)\nmemory usage: 21.9+ KB\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConverting data from one data type to another will sometimes change the data itself. Make sure to be aware of the changes you’re making! For example, changing from a float to an int might round your data to the nearest whole number.\n\n\n\n\n12.6.2 Dates and Times\nThe datetime data types in Python record years, months, days, hours, minutes, and seconds as distinct parts of a single cell of data. Pandas has many different functions and methods for dealing with time data. A datetime is a special type of numeric data that is usually not automatically detected by Pandas.\nIn the mpg data, you can change the year column to a datetime:\n\n# You must specify the format of the date or time in the existing data\n# For example: \"%m-%d-%Y\" would be the format \"05-30-2025\"\n\nmpg = mpg.assign(year = pd.to_datetime(mpg.year, format=\"%Y\"))\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\n\n\n\n\n0\naudi\na4\n1.8\n1999-01-01\n4\nauto(l5)\nf\n18\n29.0\np\ncompact\n0.450000\n\n\n1\naudi\na4\n1.8\n1999-01-01\n4\nmanual(m5)\nf\n21\n29.0\np\ncompact\n0.450000\n\n\n2\naudi\na4\n2.0\n2008-01-01\n4\nmanual(m6)\nf\n20\n31.0\np\ncompact\n0.500000\n\n\n3\naudi\na4\n2.0\n2008-01-01\n4\nauto(av)\nf\n21\n30.0\np\ncompact\n0.500000\n\n\n4\naudi\na4\n2.8\n1999-01-01\n6\nauto(l5)\nf\n16\n26.0\np\ncompact\n0.466667\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nauto(s6)\nf\n19\n28.0\np\nmidsize\n0.500000\n\n\n230\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nmanual(m6)\nf\n21\n29.0\np\nmidsize\n0.500000\n\n\n231\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nauto(l5)\nf\n16\n26.0\np\nmidsize\n0.466667\n\n\n232\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nmanual(m5)\nf\n18\n26.0\np\nmidsize\n0.466667\n\n\n233\nvolkswagen\npassat\n3.6\n2008-01-01\n6\nauto(s6)\nf\n17\n26.0\np\nmidsize\n0.600000\n\n\n\n\n234 rows × 12 columns\n\n\n\nNow the year column is a formal datetime. Because we didn’t have information on the day and month, it automatically assigned the dates to January 1st.\nTo access specific parts of a datetime object, you can use the dt namespace.\n\n# Get just the year\n\nmpg.year.dt.year\n\n0      1999\n1      1999\n2      2008\n3      2008\n4      1999\n       ... \n229    2008\n230    2008\n231    1999\n232    1999\n233    2008\nName: year, Length: 234, dtype: int32\n\n\n\n# Get just the month\n\nmpg.year.dt.month\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n229    1\n230    1\n231    1\n232    1\n233    1\nName: year, Length: 234, dtype: int32\n\n\n\n\n\n\n\n\nNote\n\n\n\nDatetimes are commonly used to create time series graphs and analyze other changes over time (see the Altair Guide for more on this). When dealing with a machine learning model, you might need to convert a datetime back to a numeric value to get it to work properly. You can get information on these uses and many more in the Pandas documentation.\n\n\n\n\n12.6.3 Text Data and Strings\nText data is distinct from categorical data. Though nominal categories often exist in the form of text, there are lots of text data (names and IDs, longer descriptions, even whole novels!) that aren’t categories.\nLike with datetime data, Pandas lets you work with its string objects through a str namespace. This lets you manipulate strings (split them, concatenate them, replace them, and more) just like you would in Python.\n\n\n\n\n\n\nTip\n\n\n\nWe’ll only show a brief example here. The main place to find information about how to work with text in Pandas is on the Text Data documentation page.\n\n\nIn the mpg dataset, the trans (short for transmission) column includes two pieces of information. You can use the str namespace to separate this info into two columns.\nFirst you can see what the split will look like:\n\n# Split on the open paranthesis to get the two parts you need\nmpg.trans.str.split(\"(\")\n\n0        [auto, l5)]\n1      [manual, m5)]\n2      [manual, m6)]\n3        [auto, av)]\n4        [auto, l5)]\n           ...      \n229      [auto, s6)]\n230    [manual, m6)]\n231      [auto, l5)]\n232    [manual, m5)]\n233      [auto, s6)]\nName: trans, Length: 234, dtype: object\n\n\nThen you can get each part of the string and assign it to new columns. (You don’t need to display the above part every time.)\n\n# Note we can use assign to define columns for each part of the split\n# Every time we manipulate a string\nmpg = mpg.assign(\n    trans = mpg.trans.str.split(\"(\").str[0], \n    transmission_full = mpg.trans.str.split(\"(\").str[1]\n)\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\ntransmission_full\n\n\n\n\n0\naudi\na4\n1.8\n1999-01-01\n4\nauto\nf\n18\n29.0\np\ncompact\n0.450000\nl5)\n\n\n1\naudi\na4\n1.8\n1999-01-01\n4\nmanual\nf\n21\n29.0\np\ncompact\n0.450000\nm5)\n\n\n2\naudi\na4\n2.0\n2008-01-01\n4\nmanual\nf\n20\n31.0\np\ncompact\n0.500000\nm6)\n\n\n3\naudi\na4\n2.0\n2008-01-01\n4\nauto\nf\n21\n30.0\np\ncompact\n0.500000\nav)\n\n\n4\naudi\na4\n2.8\n1999-01-01\n6\nauto\nf\n16\n26.0\np\ncompact\n0.466667\nl5)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nauto\nf\n19\n28.0\np\nmidsize\n0.500000\ns6)\n\n\n230\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nmanual\nf\n21\n29.0\np\nmidsize\n0.500000\nm6)\n\n\n231\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nauto\nf\n16\n26.0\np\nmidsize\n0.466667\nl5)\n\n\n232\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nmanual\nf\n18\n26.0\np\nmidsize\n0.466667\nm5)\n\n\n233\nvolkswagen\npassat\n3.6\n2008-01-01\n6\nauto\nf\n17\n26.0\np\nmidsize\n0.600000\ns6)\n\n\n\n\n234 rows × 13 columns\n\n\n\nThen you can clean up the resulting new column using the .strip() method:\n\n# The .strip() method removes text from the beginning or end of a string\n# Use .replace() if you want to remove something from anywhere in a string\nmpg = mpg.assign(transmission_full = mpg.transmission_full.str.strip(\")\"))\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\ndispl_per_cyl\ntransmission_full\n\n\n\n\n0\naudi\na4\n1.8\n1999-01-01\n4\nauto\nf\n18\n29.0\np\ncompact\n0.450000\nl5\n\n\n1\naudi\na4\n1.8\n1999-01-01\n4\nmanual\nf\n21\n29.0\np\ncompact\n0.450000\nm5\n\n\n2\naudi\na4\n2.0\n2008-01-01\n4\nmanual\nf\n20\n31.0\np\ncompact\n0.500000\nm6\n\n\n3\naudi\na4\n2.0\n2008-01-01\n4\nauto\nf\n21\n30.0\np\ncompact\n0.500000\nav\n\n\n4\naudi\na4\n2.8\n1999-01-01\n6\nauto\nf\n16\n26.0\np\ncompact\n0.466667\nl5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nauto\nf\n19\n28.0\np\nmidsize\n0.500000\ns6\n\n\n230\nvolkswagen\npassat\n2.0\n2008-01-01\n4\nmanual\nf\n21\n29.0\np\nmidsize\n0.500000\nm6\n\n\n231\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nauto\nf\n16\n26.0\np\nmidsize\n0.466667\nl5\n\n\n232\nvolkswagen\npassat\n2.8\n1999-01-01\n6\nmanual\nf\n18\n26.0\np\nmidsize\n0.466667\nm5\n\n\n233\nvolkswagen\npassat\n3.6\n2008-01-01\n6\nauto\nf\n17\n26.0\np\nmidsize\n0.600000\ns6\n\n\n\n\n234 rows × 13 columns\n\n\n\n\n\n12.6.4 Location Data\nIf your dataset has latitude and longitude values, or even specific place names, then you might be dealing with geospatial data. Geospatial data needs specialized tools, and GIS (Geographic Information Systems) is its own subfield of data science.\n\n\n\n\n\n\nImportant\n\n\n\nIf your data has only location names (or addresses) and you want to retrieve exact geocoordinates (latitude and longitude), you will need to use a process called geocoding which is built into many of the tools introduced in this section.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo work with location data, you typically need additional libraries, such as Geopandas and GeoPy, or a non-Python mapping tool like QGIS.\n\n\nLet’s use Geopandas to look at an example of data about airports. First you should import the necessary libraries: geopandas for location data analysis and vega_datasets to get some sample data.\n\nimport geopandas\nfrom vega_datasets import data\n\n\n# Read in the sample airports data\nairports = data.airports()\nairports\n\n\n\n\n\n\n\n\niata\nname\ncity\nstate\ncountry\nlatitude\nlongitude\n\n\n\n\n0\n00M\nThigpen\nBay Springs\nMS\nUSA\n31.953765\n-89.234505\n\n\n1\n00R\nLivingston Municipal\nLivingston\nTX\nUSA\n30.685861\n-95.017928\n\n\n2\n00V\nMeadow Lake\nColorado Springs\nCO\nUSA\n38.945749\n-104.569893\n\n\n3\n01G\nPerry-Warsaw\nPerry\nNY\nUSA\n42.741347\n-78.052081\n\n\n4\n01J\nHilliard Airpark\nHilliard\nFL\nUSA\n30.688012\n-81.905944\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3371\nZEF\nElkin Municipal\nElkin\nNC\nUSA\n36.280024\n-80.786069\n\n\n3372\nZER\nSchuylkill Cty/Joe Zerbey\nPottsville\nPA\nUSA\n40.706449\n-76.373147\n\n\n3373\nZPH\nZephyrhills Municipal\nZephyrhills\nFL\nUSA\n28.228065\n-82.155916\n\n\n3374\nZUN\nBlack Rock\nZuni\nNM\nUSA\n35.083227\n-108.791777\n\n\n3375\nZZV\nZanesville Municipal\nZanesville\nOH\nUSA\n39.944458\n-81.892105\n\n\n\n\n3376 rows × 7 columns\n\n\n\nOnce you have the airports data, you can create a “GeoDataFrame” that takes the latitude and longitude columns as points.\n\ngdf = geopandas.GeoDataFrame(\n    airports, geometry=geopandas.points_from_xy(airports.longitude, airports.latitude), crs=\"EPSG:4326\"\n)\ngdf\n\n\n\n\n\n\n\n\niata\nname\ncity\nstate\ncountry\nlatitude\nlongitude\ngeometry\n\n\n\n\n0\n00M\nThigpen\nBay Springs\nMS\nUSA\n31.953765\n-89.234505\nPOINT (-89.2345 31.95376)\n\n\n1\n00R\nLivingston Municipal\nLivingston\nTX\nUSA\n30.685861\n-95.017928\nPOINT (-95.01793 30.68586)\n\n\n2\n00V\nMeadow Lake\nColorado Springs\nCO\nUSA\n38.945749\n-104.569893\nPOINT (-104.56989 38.94575)\n\n\n3\n01G\nPerry-Warsaw\nPerry\nNY\nUSA\n42.741347\n-78.052081\nPOINT (-78.05208 42.74135)\n\n\n4\n01J\nHilliard Airpark\nHilliard\nFL\nUSA\n30.688012\n-81.905944\nPOINT (-81.90594 30.68801)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3371\nZEF\nElkin Municipal\nElkin\nNC\nUSA\n36.280024\n-80.786069\nPOINT (-80.78607 36.28002)\n\n\n3372\nZER\nSchuylkill Cty/Joe Zerbey\nPottsville\nPA\nUSA\n40.706449\n-76.373147\nPOINT (-76.37315 40.70645)\n\n\n3373\nZPH\nZephyrhills Municipal\nZephyrhills\nFL\nUSA\n28.228065\n-82.155916\nPOINT (-82.15592 28.22806)\n\n\n3374\nZUN\nBlack Rock\nZuni\nNM\nUSA\n35.083227\n-108.791777\nPOINT (-108.79178 35.08323)\n\n\n3375\nZZV\nZanesville Municipal\nZanesville\nOH\nUSA\n39.944458\n-81.892105\nPOINT (-81.89211 39.94446)\n\n\n\n\n3376 rows × 8 columns\n\n\n\nNow that you have a GeoDataFrame with a geometry column, you can use it any number of ways. You could find the distance between Pittsburgh’s airport and any other.\n\n# To get distances in meters, you must use .to_crs() to convert to\n# a meter-based coordinate system\n\n# First get the location of Pittsburgh International Airport\nPIT_location = gdf[gdf.iata==\"PIT\"].geometry.to_crs(10598).iloc[0]\n\n# Then use the geopandas distance method to add a new column\ngdf = gdf.assign(distance_from_PIT = gdf[\"geometry\"].to_crs(10598).distance(PIT_location))\n\n# Sort to see the closes and farthest airports\ngdf.sort_values(\"distance_from_PIT\", ascending=False)\n\n\n\n\n\n\n\n\niata\nname\ncity\nstate\ncountry\nlatitude\nlongitude\ngeometry\ndistance_from_PIT\n\n\n\n\n2794\nROP\nPrachinburi\nNaN\nNaN\nThailand\n14.078333\n101.378334\nPOINT (101.37833 14.07833)\n1.212380e+07\n\n\n2795\nROR\nBabelthoup/Koror\nNaN\nNaN\nPalau\n7.367222\n134.544167\nPOINT (134.54417 7.36722)\n1.202162e+07\n\n\n3355\nYAP\nYap International\nNaN\nNaN\nFederated States of Micronesia\n9.516700\n138.100000\nPOINT (138.1 9.5167)\n1.177099e+07\n\n\n3001\nSPN\nTinian International Airport\nNaN\nNaN\nN Mariana Islands\n14.996111\n145.621384\nPOINT (145.62138 14.99611)\n1.114018e+07\n\n\n2659\nPPG\nPago Pago International\nPago Pago\nAS\nUSA\n14.331023\n-170.710526\nPOINT (-170.71053 14.33102)\n8.644101e+06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n721\n9G1\nWest Penn\nTarentum\nPA\nUSA\n40.604233\n-79.820606\nPOINT (-79.82061 40.60423)\n3.729794e+04\n\n\n683\n8G7\nZelienople\nZelienople\nPA\nUSA\n40.801619\n-80.160729\nPOINT (-80.16073 40.80162)\n3.516677e+04\n\n\n1020\nBVI\nBeaver County\nBeaver Falls\nPA\nUSA\n40.772481\n-80.391426\nPOINT (-80.39143 40.77248)\n3.375847e+04\n\n\n792\nAGC\nAllegheny Cty\nPittsburgh\nPA\nUSA\n40.354401\n-79.930169\nPOINT (-79.93017 40.3544)\n2.955747e+04\n\n\n2626\nPIT\nPittsburgh International\nPittsburgh\nPA\nUSA\n40.491466\n-80.232871\nPOINT (-80.23287 40.49147)\n0.000000e+00\n\n\n\n\n3376 rows × 9 columns\n\n\n\nHow far is PIT from LAX?\n\ngdf[gdf.iata == \"LAX\"]\n\n\n\n\n\n\n\n\niata\nname\ncity\nstate\ncountry\nlatitude\nlongitude\ngeometry\ndistance_from_PIT\n\n\n\n\n2039\nLAX\nLos Angeles International\nLos Angeles\nCA\nUSA\n33.942536\n-118.408074\nPOINT (-118.40807 33.94254)\n3.440868e+06\n\n\n\n\n\n\n\nYou can use the same information to make an interactive map.\n\ngdf.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "guides/altair.html",
    "href": "guides/altair.html",
    "title": "13  Altair",
    "section": "",
    "text": "13.1 Category Plots\nAltair is a library for creating basic data visualization. It provides an easy to understand interface for some of the most common graph types.\nTo begin, you’ll need to import both pandas and altair. For consistency, you can import the same mpg dataset that we used in the previous chapter.\nAltair code follows the model of the Grammar of Graphics. You choose variable names (surrounded by quotes) to map to the x- and y-axis of your graph, and you can also map variables to things like Color and Column. You also set the Chart object to refer to the DataFrame you’re working with.\nTo get different kinds of visualizations, you choose from different Marks, which determine how your data will be displayed visually. In this tutorial you’ll learn some basic examples.\nCategorical plots let you compare groups according to categorical variables. A standard category plot is the bar plot, which usually compares means of different groups. In Altair, we can assign our variables to the X- and Y-axes with one categorical (nominal) and one numerical (quantitative) variable, take the mean (average) of our quantitative variable, and draw with mark_bar().\nalt.Chart(mpg, title=\"Fuel Efficiency of Drive Trains\").mark_bar().encode(\n    x=alt.X('drv:N').title(\"Drive train\"),\n    y=alt.Y('average(hwy):Q').title(\"Miles per gallon highway\"),\n)\nAbove is the code for our bar plot. We could do lots of customization from here, but this is what it will look like by default. Note that use the average() aggregate function to get the mean of our hwy variable, and we assign everything a label using title.\nYou can similarly create a box plot to compare medians and distributions among groups instead. You can use the mark_boxplot() function, and this time you don’t need to transform any of the variables.\nalt.Chart(mpg, title=\"Fuel Efficiency of Drive Trains\").mark_boxplot().encode(\n    x=alt.X('drv:N').title(\"Drive train\"),\n    y=alt.Y('hwy:Q').title(\"Miles per gallon highway\"),\n)",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Altair</span>"
    ]
  },
  {
    "objectID": "guides/altair.html#distribution-plots",
    "href": "guides/altair.html#distribution-plots",
    "title": "13  Altair",
    "section": "13.2 Distribution Plots",
    "text": "13.2 Distribution Plots\nDistribution plots show frequencies of particular variables. Distribution plots with just one variable are histograms, which require “binning” numeric variables. The Y-axis in a histogram is always a count.\n\nalt.Chart(mpg, title=\"Distribution of City Fuel Efficiency\").mark_bar().encode(\n    x=alt.X('cty:Q').bin().title('Miles per gallon city'),\n    y='count()',\n)\n\n\n\n\n\n\n\nNotice that you used the bin() function on the X variable above. You can make the same histogram into a density plot using the transform_density() function.\n\nalt.Chart(mpg, title=\"Distribution of City Fuel Efficiency\").transform_density(\n    'cty',\n    as_=['cty', 'density'],\n).mark_area().encode(\n    x=alt.X('cty:Q').title('Miles per gallon city'),\n    y=alt.Y('density:Q').title('Count of Records'),\n)\n\n\n\n\n\n\n\nDistribution plots with two variables create heatmaps. For this one you’ll need mark_rect() to create the heatmap’s boxes. You’ll also use a Color encoding to add a color scale to the boxes. Both variables need to be binned.\n\nalt.Chart(mpg, title=\"City Fuel Efficiency Related to Engine Displacement\").mark_rect().encode(\n    x=alt.X('displ:Q').bin().title('Engine displacement (gallons)'),\n    y=alt.Y('cty:Q').bin().title('Miles per gallon city'),\n    color=alt.Color('count():Q').scale(scheme='greenblue')\n)",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Altair</span>"
    ]
  },
  {
    "objectID": "guides/altair.html#relationship-plots",
    "href": "guides/altair.html#relationship-plots",
    "title": "13  Altair",
    "section": "13.3 Relationship Plots",
    "text": "13.3 Relationship Plots\nTo show a correlation or regression between two variables, use a simple scatterplot. In Altair, you draw a scatterplot’s points with mark_point(). Scatterplots take two numerical (quantitative) variables).\n\nalt.Chart(mpg, title=\"Engine Displacement and Fuel Efficiency\").mark_point().encode(\n    x=alt.X('displ:Q').title(\"Engine displacement (gallons)\"),\n    y=alt.Y('cty:Q').title(\"Miles per gallon city\"),\n)\n\n\n\n\n\n\n\nYou can separate this by color with Color encoding.\n\nalt.Chart(mpg, title=\"Engine Displacement and Fuel Efficiency\").mark_point().encode(\n    x=alt.X('displ:Q').title(\"Engine displacement (gallons)\"),\n    y=alt.Y('cty:Q').title(\"Miles per gallon city\"),\n    color=alt.Color('drv:N').title(\"Drive train\"),\n)\n\n\n\n\n\n\n\nLine plots are also a kind of relationship plot. Line plots are often used with time variables, and the mpg dataset only includes two years. To make this easier to see, we’ll use Vega’s similar cars dataset. Note that you must use an aggregate function to average the fuel efficiency by year, like you did for the bar plot.\n\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars, title=\"Model Year and Fuel Efficiency\").mark_line().encode(\n    x=alt.X('Year:T').title(\"Model Year\"),\n    y=alt.Y('mean(Miles_per_Gallon):Q').title(\"Fuel Efficiency (miles per gallon)\"),\n    color=alt.Color('Origin:N').title('Place of origin')\n)\n\n\n\n\n\n\n\nAltair also includes TimeUnit functions that you let you select specific parts of a datetime to plot.\n\ncars = cars.assign(Year = pd.to_datetime(cars.Year, format=\"%Y\"))\n\nalt.Chart(cars, title=\"Model Year and Fuel Efficiency\").mark_line().encode(\n    x=alt.X('yearmonth(Year):T').title(\"Model Year\"),\n    y=alt.Y('mean(Miles_per_Gallon):Q').title(\"Fuel Efficiency (miles per gallon)\"),\n    color=alt.Color('Origin:N').title('Place of origin')\n)\n\n\n\n\n\n\n\nYou can add a regression line to a scatter plot with the transform_regression() function. This is also called a line of best fit. You must first save the chart as a variable and then “add” the regression line to it.\n\nchart = alt.Chart(mpg, title=\"Engine Displacement and Fuel Efficiency\").mark_point().encode(\n    x=alt.X('displ:Q').title(\"Engine displacement (gallons)\"),\n    y=alt.Y('cty:Q').title(\"Miles per gallon city\"),\n) \n\nchart + chart.transform_regression('displ', 'cty').mark_line()",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Altair</span>"
    ]
  },
  {
    "objectID": "guides/altair.html#faceting",
    "href": "guides/altair.html#faceting",
    "title": "13  Altair",
    "section": "13.4 Faceting",
    "text": "13.4 Faceting\nIt sometimes makes sense to split data into separate graphs by category. The easiest way to do this is with the Column encoding.\n\nalt.Chart(mpg, title=\"Engine Displacement and Fuel Efficiency\").mark_point().encode(\n    x=alt.X('displ:Q').title(\"Engine displacement (gallons)\"),\n    y=alt.Y('cty:Q').title(\"Miles per gallon city\"),\n    color=alt.Color('drv:N').title(\"Drive train\"),\n    column=alt.Column('drv:N').title(\"Drive train\"),\n)",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Altair</span>"
    ]
  },
  {
    "objectID": "guides/altair.html#interactivity",
    "href": "guides/altair.html#interactivity",
    "title": "13  Altair",
    "section": "13.5 Interactivity",
    "text": "13.5 Interactivity\nSometimes it is useful to create plots that your reader can interact with directly, and Altair provides some simple functions for this. In scatterplots, you can add a Tooltip encoding to see what the manufacturer of the car is when you mouseover a point. You can also add the interactive() function to the end of the scatterplot code to enable scroll-to-zoom and click-and-drag features.\n\n\n\n\n\n\nTip\n\n\n\nThis is only scratching the surface of what’s possible with Altair interactivity. There’s much, much more in the Altair documentation.\n\n\n\nalt.Chart(mpg, title=\"Engine Displacement and Fuel Efficiency\").mark_point().encode(\n    x=alt.X('displ:Q').title(\"Engine displacement (gallons)\"),\n    y=alt.Y('cty:Q').title(\"Miles per gallon city\"),\n    color=alt.Color('drv:N').title(\"Drive train\"),\n    tooltip=alt.Tooltip('manufacturer:N')\n).interactive()",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Altair</span>"
    ]
  },
  {
    "objectID": "guides/hypothesis-testing.html",
    "href": "guides/hypothesis-testing.html",
    "title": "14  Hypothesis Testing",
    "section": "",
    "text": "14.1 The Steps of a Permutation Test\nThe goal of a hypothesis test is to determine whether the data in your sample accurately reflects the full population. This guide won’t get into detail about how hypothesis tests work statistically, but it will give you an outline for how to perform permutation tests with Python and Pandas.\nNo matter what your test statistic is, here are the steps you need to follow each time you want to run a permutation test. At each step, you should fully explain and interpret what you are doing.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "guides/hypothesis-testing.html#the-steps-of-a-permutation-test",
    "href": "guides/hypothesis-testing.html#the-steps-of-a-permutation-test",
    "title": "14  Hypothesis Testing",
    "section": "",
    "text": "Run some Exploratory Data Analysis on your dataset. Typically it’s helpful to make a visualization that lets you see the potential difference, correlation, or other phenomenon you want to test.\nCalculate your observed test statistic. In our case, this will usually either be a difference in means between two groups, or a Pearson’s correlation coefficient.\nBased on your exploratory analysis and test statistic, write out your null and alternative hypothesis.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the MathJax markdown syntax to produce a nicely-formatted hypothesis, so $H_0 : \\bar A = \\bar C$ will become \\(H_0 : \\bar A = \\bar C\\) and $H_1 : \\bar A \\gt \\bar C$ will become \\(H_1 : \\bar A \\gt \\bar C\\).\n\n\n\nChoose the appropriate simulation function for the hypothesis test you’re running. Make sure you have a basic understanding of how the code works in these simulations—you may have to filter or adjust your data to prepare it for the function. (See below for the functions you can choose from.)\nIn a for loop, run the simulation many times (1000 is a good minimum) and put the resulting permutations into a list. This is similar to how we conducted bootstrap sampling. Convert that list to a dataframe with two columns: the permutation and the observed statistic.\nCreate a histogram that shows your permutation distribution and your observed statistic. This is a standard histogram plus a mark_rule() layer that draws a vertical line for the observed statistic.\nCalculate the p-value (see below) and interpret the p-value and histogram together.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "guides/hypothesis-testing.html#simulation-functions",
    "href": "guides/hypothesis-testing.html#simulation-functions",
    "title": "14  Hypothesis Testing",
    "section": "14.2 Simulation Functions",
    "text": "14.2 Simulation Functions\nPermutation tests use simulation and resampling to create a random permutation as a point of comparison. Here are two simulation functions that apply to different test statistics. These functions use the same sampling techniques we learned about when we calculated confidence intervals with bootstrap sampling, but they use sampling without replacement. You’ll use these as part of steps 4 and 5 in the instructions above.\n\n\n\n\n\n\nCaution\n\n\n\nNote that these functions take specific kinds of arguments and will require the right inputs to work properly!\n\n\n\n14.2.1 Function for Simulating Difference in Means\n\n\n\n\n\n\nImportant\n\n\n\nThis function will typically require two steps before it can be run. You will need to filter your dataframe so it includes only the two groups you are testing, and you will need to calculate the number of rows in the first group.\n\n\ndef simulate_two_groups(df, group1_len, stat_var):\n    \"\"\"\n    This simulation function takes the following arguments:\n    - df: the name of your dataframe, limited to only the two groups you care about\n    - group1_len (int): the length (number of rows) in the first of your two groups of data\n    - stat_var (str): the name of the column (numerical variable) for the mean you want to test\n    \"\"\"\n    data = df[stat_var].sample(frac=1) #Reshuffle all data\n    group1 = data.iloc[:group1_len] #Get random first group\n    group2 = data.iloc[group1_len:] #Get random second group\n    return group1.mean() - group2.mean() #Calculate mean difference\n\n\n14.2.2 Function for Simulating Correlation\ndef simulate_correlation(df,var1,var2):\n    \"\"\"\n    This simulation function takes the following arguments:\n    - df: the name of your dataframe\n    - var1 (str): the name of the first column (numerical variable) you want to compare\n    - var2 (str): the name of the second column (numerical variable) you want to compare\n    \"\"\"\n    shuffled = df[var1].sample(frac=1).reset_index(drop=True)\n    corr = shuffled.corr(df[var2])\n    return corr",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "guides/hypothesis-testing.html#calculating-a-p-value",
    "href": "guides/hypothesis-testing.html#calculating-a-p-value",
    "title": "14  Hypothesis Testing",
    "section": "14.3 Calculating a p-value",
    "text": "14.3 Calculating a p-value\nIn addition to the histogram of the permutation distribution, the p-value can tell you what percentage of your randomly simulated data was more extreme than your observed result. There are a few different formulas you can use to achieve this.\n\n\n\n\n\n\nCaution\n\n\n\nIn a p-value calculation, you need all the permutations you calculated as well as your observed statistic. The different calculations below assume your dataframe of results is called results, the permutation column of that dataframe is called permutations, and the observed statistic variable is called obs_stat. Your own variable and column names may be different!\n\n\n\n14.3.1 One-tailed alternative hypothesis\nIf you have a one-tailed alternative hypothesis, simply calculate the mean of the permutations that are either greater than or equal to your observed statistic.\nIf your observed statistic is positive, you are looking for permutations greater than the observed statistic, like so:\np_value = (results.permutations &gt;= obs_stat).mean()\nIf your observed statistic is negative, you are looking for permutations less than the observed statistic:\np_value = (results.permutations &lt;= obs_stat).mean()\n\n\n\n\n\n\nTip\n\n\n\nA great rule of thumb here is: the comparison symbol (greater than or less than) should always go in the same direction as the comparison symbol in your alternative hypothesis.\n\n\n\n\n14.3.2 Two-tailed alternative hypothesis\nIf you have a two-tailed hypothesis, you should compare the absolute values of your permutations to the absolute value of your observed statistic. In this case, you are always looking for permutations greater than your observed statistic. You can use the numpy function np.abs() to get the absolute values.\np_value = (np.abs(results.permutations) &gt;= np.abs(observed_difference)).mean()",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html",
    "href": "guides/sklearn.html",
    "title": "15  Modeling with Scikit-learn",
    "section": "",
    "text": "15.1 The scikit-learn modeling workflow\nScikit-learn, or sklearn, is an all-purpose Python library for statistical modeling and machine learning. It gives you access to many different kinds of models and approaches, and we will just scratch the surface in this class. You’ll use scikit-learn any time you create a model, whether it’s a supervised regression or classification, or an unsupervised approach. This guide will walk you through the general aspects of modeling in scikit-learn, but the specifics will be up to you!\nNo matter what kind of modeling you’re doing, here are the general steps you will follow:",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#the-scikit-learn-modeling-workflow",
    "href": "guides/sklearn.html#the-scikit-learn-modeling-workflow",
    "title": "15  Modeling with Scikit-learn",
    "section": "",
    "text": "Choose your model based on your data and research questions\nChoose predictor and target variables appropriate to the model, test for validity if necessary\nPrepare the data: removing any null values, scaling/normalizing variables, or using one-hot encoding or reference coding as necessary.\nSplit the data into train and test portions (n.b.: You will skip this step for unsupervised approaches!)\nInitialize the model, paying special attention to hyperparameters\nFit the model to your training data\nSummarize and/or predict based on the model\nValidate and assess the model based on your results",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#choose-a-model-and-import-functions",
    "href": "guides/sklearn.html#choose-a-model-and-import-functions",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.2 Choose a model and import functions",
    "text": "15.2 Choose a model and import functions\nUnlike the other libraries we’ve used so far, you’d never simply run import sklearn: the library is too big and would import way more than you need, potentially slowing down your code! Instead, you’ll import just the parts you need for a particular model.\nIn the first step of the modeling workflow, you need to use the model selection principles we’ll discuss in class to determine the right model for your data and research questions. That will give you a good idea of what functions and methods you’ll need. You’ll import them with code that looks like this:\n# You only need to import the functions you will use\n# This may be different every time\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nThis is the example of what you’d need if you were going to run a linear regression. Note that you’re importing a class, the capitalized LinearRegression in the first line, that has your model itself. And you’re also importing all the additional functions you need to validate and explain that model.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#choose-predictor-and-target-variables",
    "href": "guides/sklearn.html#choose-predictor-and-target-variables",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.3 Choose predictor and target variables",
    "text": "15.3 Choose predictor and target variables\nNext you’ll choose what predictors and target variables you use. The process of choosing predictors (i.e. features) is called feature selection, and the kind of model you chose will often help you determine which predictors you use. The reverse is also true: the kind of features/predictors you have access to might help you determine which model to use.\nTypically you’d first identify the variables and then use them to select the specific X (predictors) and y (target) data you’ll use in the model.\npredictors = [\"your_first_predictor\", \"your_second_predictor\", \"your_third_predictor\", ...]\ntarget = \"your_target_variable\" #Note there are no brackets here\n\nX = your_dataframe[predictors]\ny = your_dataframe[target]\nSometimes you will want to test your predictors for validity before selecting them. In this case, you might create a potential_predictors list and use that before narrowing down to your official predictors. This is especially true for linear models like Linear Regression or Logistic Regression, when you often want to check for multicollinearity using a pairplot or correlation matrix at this stage.\n\n\n\n\n\n\nTip\n\n\n\nIn an unsupervised model, there is no target variable, so you would only need to set up predictors and an X. And in some cases, you would run your model on the entire dataset without selecting specific features as predictors.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#prepare-the-data",
    "href": "guides/sklearn.html#prepare-the-data",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.4 Prepare the data",
    "text": "15.4 Prepare the data\nThere are a few steps you might take at this stage. Some of these steps might happen before or during the previous step.\n\n\n\n\n\n\nImportant\n\n\n\nYOU WILL NOT DO EVERY ONE OF THESE STEPS EVERY TIME YOU RUN A MODEL. IT WILL ALWAYS DEPEND ON YOUR MODEL AND YOUR DATA.\n\n\n\n15.4.1 Remove null values\nIf your data has null values, this will cause errors in most models. It is helpful to remove null values from your data before you create your X and y variables. Use subset= to avoid dropping rows unnecesarily.\nyour_dataframe = your_dataframe.dropna(subset=predictors)\nIf there are null values in the target, you may need to add the target to the subset list.\n\n\n15.4.2 Encode categorical variables\nIf you have categorical variables in your predictors, you may need to change them to numerical 1s and 0s in order to run your model. You may be able to skip this for certain tree models, like the random forest.\nFor linear models (linear and logistic regression), use reference coding by dropping one of your category columns. This code will got in place of the code where you defined X in the previos section.\nX = pd.get_dummies(your_dataframe[predictors], drop_first=True)\nFor almost all other models, use one-hot encoding and don’t drop any of the reference columns.\nX = pd.get_dummies(your_dataframe[predictors])\n\n\n\n\n\n\nWarning\n\n\n\nIf you have a categorical variable with lots of individual classes, this process will create too many columns in your code. This could slow down your code, cause your kernel crash, and/or make your model unreliable. Make sure you’ve inspected the categorical variables you want to use.\n\n\n\n\n15.4.3 Normalize/Scale variables\nIf your predictors are at very different scales, this could affect the results of your model. Especially in the case of distance-based models like K-nearest neighbors or K-means clustering, it’s important to scale your variables.\nThe process of scaling works like using a model class in sklearn. First you import the scaler. There are many different ways to scale data, but we’ll typically standardize with z-scores.\nfrom sklearn.preprocessing import StandardScaler\nAfter you split the data (see below) you should standardize the training data.\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_std = scaler.transform(X_train)\nThen you would use X_train_std in place of X_train in any subsequent code.\nBefore you validate on your test data, you need to scale the test data with the same scaler.\nX_test_std = scaler.transform(X_test)\nNotice that you don’t re-fit the scaler at this step. After this you would use X_test_std in place of X_test in any subsequent code.\nBefore you cross-validate (or before you fit an unsupervised model), you will need to standardize the entirety of the unsplit data.\nscaler = StandardScaler()\nscaler.fit(X)\nX_std = scaler.transform(X)\nThen you would use X_std in place of X in any subsequent code.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#split-the-data",
    "href": "guides/sklearn.html#split-the-data",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.5 Split the data",
    "text": "15.5 Split the data\nFor all supervised models, you will split the data into training and test sets. A training set will determine the model’s coefficients, and a test set will let us see how well it works on new data that it hasn’t already seen. (For unsupervised models, since there is no training, you will skip this step and work on the entire dataset.)\nYou use the train_test_split function that we imported above to split your data.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.3, \n    random_state=0)\nThe test_size parameter determines what percentage of the data will be reserved for testing (in this case, 30%). If your dataset is very small, you may want to reserve a smaller test set so that you can use as much data as possible for training, but it’s unadvisable to choose a test set smaller than 20%.\nThe random_state parameter ensures that data split is the same every time. It doesn’t matter what number you use here, so long as it’s the same every time you run the function.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#initialize-the-model",
    "href": "guides/sklearn.html#initialize-the-model",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.6 Initialize the model",
    "text": "15.6 Initialize the model\nIn order to run a model, you must initialize the model class by assigning it to a variable. For a simple model like linear regression, it would look like this:\nyour_model_name = LinearRegression()\nMany models have hyperparameters and other settings you need to select at the time of initialization. Refer to your notes from class and the sklearn documentation for details about specific hyperparameters. As an example, initialization of a Naive Bayes model might look like:\nnaive_model = MultinomialNB(alpha=0.01, fit_prior=True)\nIn this case, the MultinomialNB class has two hyperparamters, alpha and fit_prior.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#fit-the-model",
    "href": "guides/sklearn.html#fit-the-model",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.7 Fit the model",
    "text": "15.7 Fit the model\nIn a supervised model, you will take the model variable that you initialized in the previous step and fit, i.e. run, it on your split training data.\nyour_model_name.fit(X_train_std, y_train)\nIn an unsupervised model, you will fit directly to the full data, and there is no target y variable.\nyour_model_name.fit(X)\nOr sometimes:\nresults = your_model_name.fit_transform(X)\nThe last option will fit and get results in the same function, but we’ll typically only use it for our PCA model.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#summarize-andor-predict",
    "href": "guides/sklearn.html#summarize-andor-predict",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.8 Summarize and/or predict",
    "text": "15.8 Summarize and/or predict\n\n\n\n\n\n\nCaution\n\n\n\nThis step is only for supervised models. For unsupervised models, you will often use the fit data to generate a transformation or retrieve attributes.\n\n\n\n15.8.1 Display and interpret coefficients\nThis option is only for linear models: linear and logistic regression. You can display the intercept and coefficients to better interpret the relationships in your data.\n# You'll use loops and string formatting here\nprint(f\"Intercept: {your_model_name.intercept_:.3f}\")\nfor c,p in zip(your_model_name.coef_,X.columns):\n    print(f\"Coefficient for {p}: {c:.4f}\")\n\n\n15.8.2 Tree diagrams and feature importances\nThis option is only for tree models: decision trees and the random forest.\n\nYou can use the plot_tree() function to create an informative tree diagram. See the Scikit-learn documentation for details.\nYou can display the feature importances from these models in order to interpret the results more accurately. Listing feature importances will use the your_model_name.feature_importances_ attribute and the same looping procedure as displaying coefficients (above).\n\n\n\n15.8.3 Get predictions\nSupervised models always produce predictions. In regression models, you produce numerical predictions with .predict(). We will always run these functions on the test data to get out of sample predictions.\n# For regression\npredictions = your_model_name.predict(X_test)\nIn classification models, you produce categorical predictions with .predict() and numerical probabilities with .predict_proba(). You should also get the target classes at this stage.\n# For classification\ncategories = your_model_name.classes_\nprobabilities = your_model_name.predict_proba(X_test_std)\npredictions = your_model_name.predict(X_test_std)\n\n# It's helpful to put the probabilities into a dataframe\nprobabilities = pd.DataFrame(probabilities, columns=categories)",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#validate-and-assess-your-model",
    "href": "guides/sklearn.html#validate-and-assess-your-model",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.9 Validate and assess your model",
    "text": "15.9 Validate and assess your model\nFor every kind of model, there are distinct validation and assessment steps that help you interpret your results.\n\n\n\n\n\n\nNote\n\n\n\nThere are many different validation methods, and this guide will go through a few of the ones we’ll use in this class. The sklearn documentation lists many more possibilities for different kinds of models.\n\n\n\n15.9.1 Cross-validation\nAll supervised models should be cross-validated. This runs the model multiple times over different splits of the data, to verify that your results weren’t skewed by a random split of the data.\nAt the top of your code, you should have the cross_val_score function imported. Then, no matter the model, you run the function on the entire unsplit dataset.\n# cv is the number of folds, or the repeated runs of the model\nscores = cross_val_score(your_model name, X, y, cv=5)\n# Use f-strings to print the mean and standard deviation of your result\nprint(f\"{scores.mean():.2} score with standard deviation {scores.std():.2}\")\nFor regression models, cross_val_score will generate an \\(R^2\\) score for each run of your model. For classification models, it will generate an accuracy score.\n\n\n15.9.2 Validation for Regression\nRegression validation is based on residuals. Here are the sklearn functions you need to validate a regression. You would add these at the top of your notebook with your other imports.\nfrom sklearn.metrics import mean_squared_error, r2_score\nYou first step in regression validation is to calculate the residuals.\n# Out-of-sample residuals:\nresiduals = y_test - predictions\n\n# It's helpful to put them into a dataframe with your predictions\nresults = pd.DataFrame({'Predictions': predictions, 'Residuals':residuals})\nYou can use the predictions you generated in the previous step to calculate Root Mean Squared Error (RMSE).\nprint(f\"Root mean squared error: {np.sqrt(mean_squared_error(y_test, predictions)):.2f}\")\nYou can also display the \\(R^2\\) score, aka the coefficient of determination.\nprint(f\"Coefficient of determination: {r2_score(y_test, predictions):.2f}\")\nIn addition to these two values, you can also create visualizations for validation.\nA histogram of residuals will show you if the residuals are normally distributed.\nalt.Chart(results, title=\"Histogram of Residuals\").mark_bar().encode(\n    x=alt.X('Residuals:Q', title=\"Residuals\").bin(maxbins=20),\n    y=alt.Y('count():Q', title=\"Value Counts\")\n)\nAnd you can plot the absolute value of residuals against the predictions to test for heteroskedasticity. You will only use this plot for linear regression.\n# Plot the absolute value of residuals against the predicted values\nchart = alt.Chart(results, title=\"Testing for Heteroskedasticity\").mark_point().encode(\n    x=alt.X('Predictions:Q').title(\"Predicted Values\"),\n    y=alt.Y('y:Q').title(\"Absolute value of Residuals\") \n).transform_calculate(y='abs(datum.Residuals)')\n\nchart + chart.transform_loess('Predictions', 'y').mark_line()\n\n\n15.9.3 Validation for Classification\nClassification validation is based on the confusion matrix, which tallies the number of categories correctly predicted by the model. Here are the sklearn functions you need to validate a classifier. You would add these at the top of your notebook with your other imports.\nfrom sklearn.metrics import confusion_matrix, classification_report, RocCurveDisplay\n# You won't always need matplotlib, but it's good to have\nimport matplotlib.pyplot as plt\nThe confusion_matrix function generates the results, and you can turn this into a confusion matrix visualization: a special kind of heatmap with text overlayed.\n# Calculate confusion matrix and transform data\nconf_mat = confusion_matrix(y_test,predictions)\nconf_mat = pd.DataFrame(conf_mat,index=categories,columns=categories)\nconf_mat = conf_mat.melt(ignore_index=False).reset_index()\n# Create heatmap\nheatmap = alt.Chart(conf_mat).mark_rect().encode(\n    x=alt.X(\"variable:N\").title(\"Predicted Response\"),\n    y=alt.Y(\"index:N\").title(\"True Response\"),\n    color=alt.Color(\"value:Q\", legend=None).scale(scheme=\"blues\")\n).properties(\n    width=400,\n    height=400\n)\n# Add text labels for numbers\ntext = heatmap.mark_text(baseline=\"middle\").encode(\n    text=alt.Text(\"value:Q\"),\n    color=alt.value(\"black\"),\n    size=alt.value(50)\n)\n\nheatmap + text\nYou can calculate the accuracy, precision, and recall scores with the classification report.\n# You must use print() to make this readable\nprint(classification_report(y_test, predictions))\nDon’t forget that you must also use cross-validation for a classifier!\nYou can also create an ROC Curve if you have a binary classifer (just two target classes).\n# Create our ROC Curve plot\nRocCurveDisplay.from_predictions(y_test,\n                                 probabilities[categories[0]],\n                                 pos_label=categories[0])\n\n# Draw a green line for 0\nplt.plot([0, 1], [0, 1], color = 'g')\n\n\n15.9.4 Validation for Unsupervised Models\nValidation steps for unsupervised models are specific to the model. For example, in K-means clustering, you could show the labels of your clustering and a bar plot of the cluster means. Refer back to our class notes and supplementary readings for validation steps for K-means clustering and PCA.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#examples-of-model-code",
    "href": "guides/sklearn.html#examples-of-model-code",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.10 Examples of model code",
    "text": "15.10 Examples of model code\n\n15.10.1 Linear regression\nHere’s an example of the full model workflow for a linear regression of the cars data.\nFirst import libraries and data. In a real scenario, you’d also do some exploratory data analysis here.\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nfrom vega_datasets import data\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nalt.data_transformers.enable(\"vegafusion\")\n\ncars = data.cars()\n\nBecause this is a linear model, you should also validate any potential predictors by testing for multicollinearity. One way to do that is to use a correlation matrix of potential predictors. (You might also use a pair plot here.)\n\npotential_predictors = ['Cylinders', 'Displacement', 'Horsepower',\n       'Weight_in_lbs', 'Acceleration']\n\n# Re-arrange correlation matrix data\ncars_corr = (cars[potential_predictors].corr(numeric_only=True)\n             .stack()\n             .reset_index()\n             .rename(columns={0:'corr','level_0':'var1','level_1':'var2'})\n            )\n# Create correlation heatmap\nbase = alt.Chart(cars_corr, title=\"Cars Correlation Matrix\").mark_rect().encode(\n    x=alt.X(\"var1:N\",title=None),\n    y=alt.Y(\"var2:N\",title=None),\n    color=alt.Color(\"corr\",title=\"Correlation coefficient\").scale(scheme='blueorange')\n).properties(width=300,height=300)\n# Add text labels for coefficients\ntext = base.mark_text(baseline='middle').encode(\n    alt.Text('corr:Q', format=\".2f\"),\n    color=alt.condition(\n        (alt.datum.corr &lt; -0.5) | (alt.datum.corr &gt; 0.5),\n        alt.value('white'),\n        alt.value('black')\n    )\n)\nbase+text # Display visualization\n\n\n\n\n\n\n\nThen set up and run your model. Since this is a linear model, you also should display and interpret the coefficients.\n\n# Select target and predictors based on validation\ntarget = \"Miles_per_Gallon\" # Our target variable\npredictors = [\"Displacement\", \"Horsepower\", \"Acceleration\"] # A list of predictors\n\n# Remove null values\ncars = cars.dropna(subset=predictors+[target])\n\n# Create variables and split data\nX = cars[predictors]\ny = cars[target]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.4, \n    random_state=42)\n\n# Fit the model\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n# View coefficients\nprint(f\"Intercept: {linreg.intercept_:.3f}\")\nfor c,p in zip(linreg.coef_,X.columns):\n    print(f\"Coefficient for {p}: {c:.4f}\")\n\nIntercept: 45.819\nCoefficient for Displacement: -0.0364\nCoefficient for Horsepower: -0.0968\nCoefficient for Acceleration: -0.3074\n\n\nFinally, predict with the model, then display and interpret the validation steps.\n\npredictions = linreg.predict(X_test)\n\n# Our out-of-sample residuals:\nresiduals = y_test - predictions\nresults = pd.DataFrame({'Predictions': predictions, 'Residuals':residuals})\n\nprint(f\"Root mean squared error: {np.sqrt(mean_squared_error(y_test, predictions)):.2f}\")\nprint(f\"Coefficient of determination: {r2_score(y_test, predictions):.2f}\")\n\nRoot mean squared error: 4.71\nCoefficient of determination: 0.59\n\n\n\n# Histogram of residuals\nalt.Chart(results, title=\"Histogram of Residuals\").mark_bar().encode(\n    x=alt.X('Residuals:Q', title=\"Residuals\").bin(maxbins=20),\n    y=alt.Y('count():Q', title=\"Value Counts\")\n)\n\n\n\n\n\n\n\n\n# Test for heteroskedasticity\n# Plot the absolute value of residuals against the predicted values\nchart = alt.Chart(results, title=\"Testing for Heteroskedasticity\").mark_point().encode(\n    x=alt.X('Predictions:Q').title(\"Predicted Values\"),\n    y=alt.Y('y:Q').title(\"Absolute value of Residuals\") \n).transform_calculate(y='abs(datum.Residuals)')\n\nchart + chart.transform_loess('Predictions', 'y').mark_line()\n\n\n\n\n\n\n\n\nscores = cross_val_score(linreg, X, y, cv=5)\nprint(f\"{scores.mean():.2} r-squared score with standard deviation {scores.std():.2}\")\n\n0.27 r-squared score with standard deviation 0.52\n\n\nRemember, you will also need to explain and interpret all of your output. For guidance on how you should do that, refer to the How To Explain guide.\n\n\n15.10.2 KNN Classification\nHere’s an example of a full model workflow for a KNN classifier of the penguins data.\nFirst import libraries and data. In a real scenario, you’d also do some exploratory data analysis here.\n\n# You'll also need pandas, numpy, and altair\n# For standardization\nfrom sklearn.preprocessing import StandardScaler\n# For KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, RocCurveDisplay\n\n# You'll also need matplotlib this time\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"../data/penguins.csv\")\n\nThen set up and run your model. Remember, there are steps here specific to this model and dataset.\n\n# First you must split and standardize the data with a new target.\n# Decide on your predictors and targets\npredictors = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"sex\"]\ntarget = \"species\" # A categorical target now\n\npenguins = penguins.dropna()\nX = pd.get_dummies(penguins[predictors])\ny = penguins[target]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.3, \n    random_state=0)\n\n# Standardizing using the training data\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_std = scaler.transform(X_train)\n\n# Fit the classification model\n# Decide on a good value for K (n_neighbors)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_std, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n5\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\nIn a distance-based model like KNN, there are no coefficients, so you can move on to the prediction and validation steps.\n\n# Get the prediction results\n\n# Standardize test data\nX_test_std = scaler.transform(X_test)\n\n# Prediction probabilities\nprobabilities = knn.predict_proba(X_test_std)\n# The predictions themselves\npredictions = knn.predict(X_test_std)\n# The categories or classes we predicted\ncategories = knn.classes_\n\n# Let's make the probabilities look nicer\nprobabilities = pd.DataFrame(probabilities, columns=categories)\nprobabilities\n\n\n\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\n\n\n\n\n0\n1.0\n0.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n\n\n3\n1.0\n0.0\n0.0\n\n\n4\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n\n\n95\n1.0\n0.0\n0.0\n\n\n96\n0.0\n0.0\n1.0\n\n\n97\n1.0\n0.0\n0.0\n\n\n98\n0.0\n0.0\n1.0\n\n\n99\n0.0\n0.0\n1.0\n\n\n\n\n100 rows × 3 columns\n\n\n\n\n# Print the classification report\nprint(classification_report(y_test, predictions))\n\n# Calculate confusion matrix and transform data\nconf_mat = confusion_matrix(y_test,predictions)\nconf_mat = pd.DataFrame(conf_mat,index=categories,columns=categories)\nconf_mat = conf_mat.melt(ignore_index=False).reset_index()\n# Create heatmap\nheatmap = alt.Chart(conf_mat).mark_rect().encode(\n    x=alt.X(\"variable:N\").title(\"Predicted Response\"),\n    y=alt.Y(\"index:N\").title(\"True Response\"),\n    color=alt.Color(\"value:Q\", legend=None).scale(scheme=\"blues\")\n).properties(\n    width=400,\n    height=400\n)\n# Add text labels for numbers\ntext = heatmap.mark_text(baseline=\"middle\").encode(\n    text=alt.Text(\"value:Q\"),\n    color=alt.value(\"black\"),\n    size=alt.value(50)\n)\n\nheatmap + text\n\n              precision    recall  f1-score   support\n\n      Adelie       0.96      1.00      0.98        48\n   Chinstrap       1.00      0.88      0.93        16\n      Gentoo       1.00      1.00      1.00        36\n\n    accuracy                           0.98       100\n   macro avg       0.99      0.96      0.97       100\nweighted avg       0.98      0.98      0.98       100\n\n\n\n\n\n\n\n\n\n\n# Run cross-validation on standardized data\nscaler_full = StandardScaler()\nscaler_full.fit(X)\n\nX_std = scaler.transform(X)\n\nscores = cross_val_score(knn, X_std, y, cv=10)\nprint(f\"{scores.mean():.2} accuracy with standard deviation {scores.std():.2}\")\n\n1.0 accuracy with standard deviation 0.0091\n\n\nSince this is not a binary classifier (i.e. We have three target classes: Adelie, Chinstrap, and Gentoo.), it’s not appropriate to include an ROC Curve here, but you may want it for a different model or dataset.\nRemember, you will also need to explain and interpret all of your output. For guidance on how you should do that, refer to the How To Explain guide.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/sklearn.html#appendix-import-models",
    "href": "guides/sklearn.html#appendix-import-models",
    "title": "15  Modeling with Scikit-learn",
    "section": "15.11 Appendix: Import Models",
    "text": "15.11 Appendix: Import Models\nHere are the bits of sklearn you will need to import different model classes. You can refer to the documentation for each of these classes to get more details about hyperparameters and other features. Remember, you should only import the code you need for your specific model.\n# Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# KNN Regression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Random Forest Regression\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Random Forest Classification\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# K-Means Clustering\nfrom sklearn.cluster import KMeans\n\n# Principal Component Analysis\nfrom sklearn.decomposition import PCA",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling with Scikit-learn</span>"
    ]
  },
  {
    "objectID": "guides/debugging.html",
    "href": "guides/debugging.html",
    "title": "16  Debugging Your Code",
    "section": "",
    "text": "16.1 Some steps to try when you encounter an error\nBugs in your code can be frustrating! But there are concrete steps you can take:",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Debugging Your Code</span>"
    ]
  },
  {
    "objectID": "guides/debugging.html#some-steps-to-try-when-you-encounter-an-error",
    "href": "guides/debugging.html#some-steps-to-try-when-you-encounter-an-error",
    "title": "16  Debugging Your Code",
    "section": "",
    "text": "16.1.1 Step 1: Define the Problem.\nThink about what you were trying to do vs. what happened instead. Create a hypothesis for what went wrong.\n\n\n16.1.2 Step 2: Read the Error Message.\nLook for a line number where the error is occurring.\nSometimes the bug is in the line before the one that threw the error!\nSee Section 16.3 below for more.\n\n\n16.1.3 Step 3: Re-run code from the beginning.\nRemember that code is run in order, from the first line in your script to the last one. And specific parts of your code, like for loops or functions, have specific indented contexts. If any of your code is out of order or indented improperly, the rest of the code may not work.\nThis can happen a lot in JupyterHub, since sometimes you re-run a lower cell without fixing one that’s higher up in your code. Go back to the beginning of your code and re-run to see if that will fix it, and if not go cell-by-cell through the code to find the problem.\n\n\n16.1.4 Step 4: Talk it out!\nTry your best to explain the problem out loud, preferably to a friend or teammate.\nPractice rubber duck debugging.\n\n\n\n16.1.5 Step 5: Check instructions, documentation, and Google.\nIf you’re lost, refer to all the resources you have: cheatsheets, guides, online documentation. Don’t forget RAD CAT!\nAnd when in doubt: Google the error message and see if someone else had the same problem!\n\n\n16.1.6 Step 6: Ask for help!\nDon’t let a single bug frustrate you for too long. If none of the above strategies worked, ask a classmate, PAL tutor, or instructor for help with the problem.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Debugging Your Code</span>"
    ]
  },
  {
    "objectID": "guides/debugging.html#avoid-bugs-before-they-happen",
    "href": "guides/debugging.html#avoid-bugs-before-they-happen",
    "title": "16  Debugging Your Code",
    "section": "16.2 Avoid Bugs before they happen!",
    "text": "16.2 Avoid Bugs before they happen!\n\n16.2.1 Save and/or “Restart and Run All” Often.\nIf you’re getting the same bug repeatedly in the same cell despite making changes, it could be that something is “stuck” in memory. Use the “restart and run all” button to try it again with a fresh kernel.\n\n\n16.2.2 Use good names.\nName your variables and dataframes with care. Rename things to make them more clear. Good names can help you find a problem quickly.\n\n\n16.2.3 Start simple, and build up little by little.\nDon’t try to write a whole program all in one go.\n\n\n16.2.4 Run your code line-by-line.\nCheck that it works as you go.\n\n\n16.2.5 Leave yourself good annotations and comments!\nUse # to leave comments: remind yourself what certain lines of code are doing.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Debugging Your Code</span>"
    ]
  },
  {
    "objectID": "guides/debugging.html#sec-anatomy",
    "href": "guides/debugging.html#sec-anatomy",
    "title": "16  Debugging Your Code",
    "section": "16.3 Anatomy of an Error Message",
    "text": "16.3 Anatomy of an Error Message\nHere’s an example of an error message from Pandas:\n\n# Import library\nimport pandas as pd\n\n# Read in data\npenguins = pd.read_csv(\"../data/penguins.csv\")\n\n# Filter only Adelie penguins\npenguins_filter = penguins[penguins.Species == \"Adelie\"]\npenguins_filter\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/var/folders/xk/29clchk51j5bm56kqn4v8dsc0000gq/T/ipykernel_11958/490271837.py in ?()\n      4 # Read in data\n      5 penguins = pd.read_csv(\"../data/penguins.csv\")\n      6 \n      7 # Filter only Adelie penguins\n----&gt; 8 penguins_filter = penguins[penguins.Species == \"Adelie\"]\n      9 penguins_filter\n\n~/CIS241/.venv/lib/python3.12/site-packages/pandas/core/generic.py in ?(self, name)\n   6314             and name not in self._accessors\n   6315             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6316         ):\n   6317             return self[name]\n-&gt; 6318         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'Species'\n\n\n\nThis code uses a lot of best practices (clear variable names, good comments, etc.), but it still has an error. Let’s see if you can decode it.\nThe bottom line of the error message defines the error itself. Make sure you always scroll to the bottom of the error message first!\nIn this case the last line says AttributeError: 'DataFrame' object has no attribute 'Species'. Before the colon is the type of error (an attribute error in Pandas refers to a column or attribute of the data), and after the colon Python attempts to tell you what’s wrong (in this case it says that the dataframe doesn’t have a column called “Species”).\nEverything above the bottom line of the error message is part of the Traceback, which shows you step by step what went wrong with the code. Sometimes these tracebacks can be very long, but usually you only need a small part of it.\nAfter you’ve looked at the last line, go back to the top part of the Traceback. This will show you the code that you wrote. In our case it looks like this:\n&lt;ipython-input-3-ff22716e7d6d&gt; in ?()\n      4 # Read in data\n      5 penguins = pd.read_csv(\"../data/penguins.csv\")\n      6 \n      7 # Filter only Adelie penguins\n----&gt; 8 penguins_filter = penguins[penguins.Species == \"Adelie\"]\nThe Traceback has an arrow that points directly at the line causing the problem. In this case, that’s line 8, the final line of our cell. Sometimes the Traceback will even highlight the part of the line that’s causing the problem!\nThere’s no highlight here, but the error definition told us what was wrong: the “Species” column. Let’s look at the columns in this DataFrame to see if we can spot the issue:\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nDo you see it?\nYes! The column isn’t called “Species”, it’s called “species”. The code accidentally capitalized the word! This is a very easy fix. You can repair the cell like so:\n\n# Import library\nimport pandas as pd\n\n# Read in data\npenguins = pd.read_csv(\"../data/penguins.csv\")\n\n# Filter only Adelie penguins\npenguins_filter = penguins[penguins.species == \"Adelie\"]\npenguins_filter\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nAdelie\nDream\n36.6\n18.4\n184.0\n3475.0\nFemale\n\n\n148\nAdelie\nDream\n36.0\n17.8\n195.0\n3450.0\nFemale\n\n\n149\nAdelie\nDream\n37.8\n18.1\n193.0\n3750.0\nMale\n\n\n150\nAdelie\nDream\n36.0\n17.1\n187.0\n3700.0\nFemale\n\n\n151\nAdelie\nDream\n41.5\n18.5\n201.0\n4000.0\nMale\n\n\n\n\n152 rows × 7 columns\n\n\n\nAnd goodbye error message!\nOver time, you’ll get used to reading and decoding these error messages on your own, but don’t be afraid to ask questions if you run into one you’re unsure about.",
    "crumbs": [
      "Home",
      "Guides",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Debugging Your Code</span>"
    ]
  },
  {
    "objectID": "how-to-explain.html",
    "href": "how-to-explain.html",
    "title": "17  How to Explain in CIS241",
    "section": "",
    "text": "17.1 How to Explain a Visualization\nThis file includes code and examples for explaining graphs and statistical output in DA101. Communicating results is a crucial part of good data analysis, and we try to communicate all results completely and accurately and in terms of the data.\nThese short examples are designed to give you general guidance. I cannot provide a comprehensive example or answer that you could “copy” every time to have an A+ explanation, but I can provide an example and some pointers to help you get started.\n(This is adapted from “How to explain in DA101”.)\nThe way you explain your graphs will change throughout the semester as you learn more details about what the graph shows and also learn more technical lingo for how to identify different aspects of the graph, including visual interpretation of summary statistics, and how to identify potentially significant differences or outliers.\nIn the beginning of class (let’s say weeks 1-2) I won’t assume you have prior technical knowledge of data analysis, and it is okay to stick to general descriptive and observational descriptions of what you’re seeing in a graph that you make. What are you noticing? What stands out to you? Do you see anything that looks like a pattern in the points or that indicates similarity among groups?\nLater on (let’s say weeks 3+) you will increasingly gain technical language to be able to talk about your graphs and describe your observations. As you gain these skills you can still describe what you are noticing and seeing in your graphs, but you will increasingly describe summary statistics…",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>How to Explain in CIS241</span>"
    ]
  },
  {
    "objectID": "how-to-explain.html#how-to-explain-a-visualization",
    "href": "how-to-explain.html#how-to-explain-a-visualization",
    "title": "17  How to Explain in CIS241",
    "section": "",
    "text": "17.1.1 Explaining a Boxplot\n\n\nCode\nimport pandas as pd\nimport altair as alt\nimport numpy as np\n\nmpg = pd.read_csv(\"workshops/sample_data/mpg.csv\")\nmpg.drop(\"Unnamed: 0\",axis=1,inplace=True)\n\n\n\n\nCode\nalt.Chart(mpg, title=\"Fuel Efficiency Among Different Vehicle Types\").mark_boxplot(size=40).encode(\n    x=alt.X(\"class:N\", title=\"Type or Class of Vehicle\"),\n    y=alt.Y(\"hwy:Q\", title=\"Highway miles per gallon\"),\n    color=alt.Color(\"class:N\", legend=None)\n).properties(width=500)\n\n\n\n\n\n\n\n\n\n17.1.1.1 Basic Explanation (Weeks 1-2)\nThis boxplot of the mpg data set shows the distributions of highway fuel efficiency across the seven different kinds of cars in the data. Pickups and SUVs seem to have lower fuel efficiency than the other cars, which makes sense because they are bigger, heavier vehicles. Smaller vehicles like compact and midsize cars have greater fuel efficiency, and subcompacts have similarly high fuel efficiency but the data seems to be more spread out because the box is longer. Overall it seems like vehicle class is related to fuel efficiency, with smaller cars tending to have greater efficiency.\n\n\n17.1.1.2 Detailed Explanation (Weeks 3+)\nThis boxplot of the mpg data set shows the distributions of highway fuel efficiency across the seven different kinds of cars in the data. Pickups and SUVs have medians and interquartile ranges well below the other vehicles, suggesting a statistically significant difference. Compact cars have a nearly identical IQR and median to midsize cars, as evidenced by the size of the two boxes, though there are a handful of outliers in the compact group. Subcompacts have a larger interquartile range than any of the other groups, which suggests greater variability in their fuel efficiency distribution. Overall the graph suggests that larger vehicle classes tend to have lower fuel efficiency distributions, while smaller vehicles seem to have greater fuel efficiency.\n\n\n\n17.1.2 Explaining a Scatterplot\n\n\nCode\nalt.Chart(mpg, title=\"Relationship Between Engine Size and City Fuel Efficiency\").mark_point().encode(\n    x=alt.X(\"displ:Q\", title=\"Engine Displacement, in liters\").scale(zero=False),\n    y=alt.Y(\"cty:Q\", title=\"City miles per gallon\").scale(zero=False)\n).properties(width=500).interactive()\n\n\n\n\n\n\n\n\n\n17.1.2.1 Basic Explanation (Weeks 1-2)\nThis scatter plot of the mpg data set shows the relationship between the size of a car’s engine (using the engine displacement variable) and a car’s city fuel efficiency. Because of the downward slope of the dots as the graph goes from left to right, it appears that as engines get bigger the city fuel efficiency gets smaller. After about 4.5 liters the slope levels off, suggesting there isn’t as strong a relationship past this point. Overall, we could conclude that a car’s city fuel efficiency may partially depend on the size of the engine.\n\n\n17.1.2.2 Detailed Explanation (Weeks 3+)\nThis scatter plot of the mpg data set shows the relationship between the size of a car’s engine (using the engine displacement variable) and a car’s city fuel efficiency. There looks to be a negative correlation between the two variables: as engine displacement goes up, city miles per gallon goes down. Adding a line of best fit to this graph or calculating a correlation coefficient would give us a better indication of the possible correlation. After about 4.5 liters, the points no longer slope downward, which may indicate that after a certain threshold, engine displacement has no direct correlation to fuel efficiency. Overall we could conclude that our dependent variable, city miles per gallon, negatively correlates with our independent variable, engine displacement, and therefore that as engine size gets larger fuel efficiency drops.",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>How to Explain in CIS241</span>"
    ]
  },
  {
    "objectID": "how-to-explain.html#how-to-explain-statistical-output",
    "href": "how-to-explain.html#how-to-explain-statistical-output",
    "title": "17  How to Explain in CIS241",
    "section": "17.2 How to explain statistical output",
    "text": "17.2 How to explain statistical output\nWe will learn several statistical tests and models throughout the semester. In data analysis, there is much more to do than to simply write the code for the model and generate “correct” output or report a p-value. In most cases, explaining the output and validation from the models will require several sentences that help to translate the quantitative results in terms of the data. In general, when running these models and interpreting output, there are a few key things to keep in mind.\n\nDo you have a logical reason for running the test or model?\nAfter you’ve run the code, were you able to calculate and report the key values from the statistical output?\nAfter you have identified and reported the key values, can you connect them back to the data and the question at hand?\nFinally, can you describe the results in terms of statistical and practical significance?\n\nI’ll provide a few examples below to walk through a t-test, a correlation test, and a linear regression. These are not “perfect” or “set in stone” formats for explaining, but rather think of them as an aid to thought to help guide you in your journey of learning how to explain and translate like a data analyst.\n\n17.2.1 Explaining a permutation test for comparing means\n\n\nCode\ndef simulate_two_groups(data1, data2):\n    n, m = len(data1), len(data2)\n    data = np.append(data1, data2)\n    np.random.shuffle(data)\n    group1 = data[:n]\n    group2 = data[n:]\n    return group1.mean() - group2.mean()\n\ncompact_hwy = mpg[mpg[\"class\"] == \"compact\"].hwy\nmidsize_hwy = mpg[mpg[\"class\"] == \"midsize\"].hwy\nprint(f\"Mean mpg of compact: {compact_hwy.mean():.2f}\")\nprint(f\"Mean mpg of midsize: {midsize_hwy.mean():.2f}\")\n\nobserved_diff = compact_hwy.mean()-midsize_hwy.mean()\nprint(f\"Difference in means of compact and midsize cars: {observed_diff:.3f} miles per gallon\")\n\nmean_perms = pd.DataFrame({\"mean_perms\":[simulate_two_groups(compact_hwy,midsize_hwy) for i in range(10000)]})\n\n\nMean mpg of compact: 28.30\nMean mpg of midsize: 27.29\nDifference in means of compact and midsize cars: 1.005 miles per gallon\n\n\n\n\nCode\nalt.data_transformers.disable_max_rows() # Don't limit the data\n# Create a histogram\nhistogram = alt.Chart(mean_perms).mark_bar().encode(\n    x=alt.X(\"mean_perms:Q\").bin(maxbins=50),\n    y=alt.Y(\"count():Q\")\n).properties(width=500)\nmean_perms = mean_perms.assign(mean_diff=observed_diff) # Add the mean to the dataframe\n# Add a vertical line\nobserved_difference = alt.Chart(mean_perms).mark_rule(color=\"red\", strokeDash=(8,4)).encode(\n    x=alt.X(\"mean_diff\")\n)\n# Combine the two plots\nhistogram + observed_difference\n\n\n\n\n\n\n\n\n\n\nCode\np_value = np.mean(mean_perms.mean_perms &gt; observed_diff)\nprint(f\"p-value = {p_value}\")\n\n\np-value = 0.0641\n\n\nThe permutation test suggests there is no significant difference in the mean highway miles per gallon of the midsize and compact vehicle classes (p=0.06), though the p-value is very close to 0.05. I was expecting this because these vehicles are very similar in size and because their ranges seem to overlap on the boxplot. The true difference in the means is 1.005 miles per gallon, which doesn’t seem like very much. The mean highway miles per gallon used for compact cars was 28.30 mpg and the mean for midsize cars was 27.29 mpg. The result is not statistically significant, and it’s not practically significant either (1 more mile per gallon doesn’t seem like that much greater fuel efficiency).\n\n\n17.2.2 Explaining a Linear Regression\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nTarget variable: miles per gallon city (cty)\nPredictor variables: engine displacement (displ), model year (year), number of cylinders (cyl), vehicle class (class)\n\n\nCode\ntarget = \"cty\"\npredictors = [\"displ\", \"year\", \"cyl\", \"class\"]\n\nalt.Chart(mpg).mark_point().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    row=[\"displ\", \"year\", \"cyl\"],\n    column=[\"displ\", \"year\", \"cyl\"]\n)\n\n\n\n\n\n\n\n\n\n\nCode\nmpg[predictors].corr(numeric_only=True)\n\n\n\n\n\n\n\n\n\ndispl\nyear\ncyl\n\n\n\n\ndispl\n1.000000\n0.147843\n0.930227\n\n\nyear\n0.147843\n1.000000\n0.122245\n\n\ncyl\n0.930227\n0.122245\n1.000000\n\n\n\n\n\n\n\nThe pairplot and correlation matrix above show correlations for the three numerical predictor variables I chose (engine displacement, model year, and number of cylinders). The last predictor variable, vehicle class, was excluded because it is categorical. As you can see from the steep regression line in the pairplot and the high correlation coefficient of 0.93, engine displacement and number of cylinders are highly correlated. It wouldn’t be valid to use both in a regression model, so I will exclude number of cylinders going forward and use only engine displacement, model year, and vehicle class.\n\n\nCode\npredictors = [\"displ\", \"year\", \"class\"]\n\nX = pd.get_dummies(mpg[predictors], drop_first=True)\nY = mpg[target]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    Y, \n    test_size=0.4, \n    random_state=0)\n\nour_model = LinearRegression()\nour_model.fit(X_train, y_train)\n\nprint(f\"Intercept: {our_model.intercept_:.3f}\")\nfor c,p in zip(our_model.coef_,X.columns):\n    print(f\"Coefficient for {p}: {c:.4f}\")\n\n\nIntercept: -50.633\nCoefficient for displ: -2.1743\nCoefficient for year: 0.0399\nCoefficient for class_compact: -4.4195\nCoefficient for class_midsize: -4.3175\nCoefficient for class_minivan: -6.2129\nCoefficient for class_pickup: -6.6474\nCoefficient for class_subcompact: -3.6546\nCoefficient for class_suv: -5.7987\n\n\n\n\nCode\npredictions = our_model.predict(X_test)\nresiduals = y_test - predictions\nprint(f\"Root mean squared error: {np.sqrt(mean_squared_error(y_test, predictions)):.2f}\")\nprint(f\"Coefficient of determination (R-squared): {r2_score(y_test, predictions):.2f}\")\n\n\nRoot mean squared error: 2.59\nCoefficient of determination (R-squared): 0.69\n\n\nThis linear regression model looks at the effect engine displacement in liters, the year the vehicle was made, and the type or class of vehicle have on city miles per gallon fuel efficiency. The coefficient for engine displacement suggests a negative relationship: for each liter of engine displacement, city miles per gallon decreases by 2.17. The coefficient for model year suggest that fuel efficiency increases very slightly (0.04 mpg) for each year in which the car is made. Compared to our “baseline” category of a two-seater car, all other vehicle classes have lower fuel efficiency. All of these coefficients make sense: we would expect newer cars that are smaller (like a two-seater with low engine displacement) to have greater fuel efficiency.\n\\(R^2\\) is .69, suggesting that 69% of the variation in city fuel efficiency is accounted for by engine displacement. I am unsure if this result is practically signficant. For a mechanical process like the fuel efficiency of an engine, we might expect to see an \\(R^2\\) higher than 67%. The root mean squared error is 2.59, meaning that the residuals are off on average by more than 2 miles per gallon. This seems like a fair amount and raises some questions about the accuracy of the model.\n\n\nCode\nresults = pd.DataFrame({'Predictions': predictions, 'Residuals':residuals})\n\nalt.Chart(results, title=\"Histogram of Residuals\").mark_bar().encode(\n    x=alt.X('Residuals:Q', title=\"Residuals\").bin(maxbins=20),\n    y=alt.Y('count():Q', title=\"Value Counts\")\n).properties(width=500)\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of residuals for the model. While it appears that the residuals are centered near 0, there are some outliers to the right of the graph that prevent the residuals from having a normal distribution. This suggests that our model may not be totally reliable.\n\n\nCode\n# Plot the absolute value of residuals against the predicted values\nchart = alt.Chart(results, title=\"Testing for Heteroskedasticity\").mark_point().encode(\n    x=alt.X('Predictions:Q').title(\"Predicted Values\").scale(zero=False),\n    y=alt.Y('y:Q').title(\"Absolute value of Residuals\") \n).transform_calculate(y='abs(datum.Residuals)').properties(width=500)\n\nchart + chart.transform_loess('Predictions', 'y').mark_line()\n\n\n\n\n\n\n\n\nThe above plot shows the predicted values plotted against the absolute value of the residuals. Like in the histogram of residuals, we see a few outliers that are slightly skewing the results. But overall the line across the plot is mostly horizontal, suggesting that we do not see much heteroskedasticity in our model. While our model may not be ideal, it is probably valid.",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>How to Explain in CIS241</span>"
    ]
  },
  {
    "objectID": "radcat.html",
    "href": "radcat.html",
    "title": "18  R.A.D. C.A.T.",
    "section": "",
    "text": "18.1 Let’s get started\nThis is the CIS department’s guide to following instructions for any assignment.\nFor your course, be aware of the various places that you might find information about expectations for completing and submitting assignments:\n[Image credit: Vinayakvel, CC BY-SA 4.0, via Wikimedia Commons]",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>R.A.D. C.A.T.</span>"
    ]
  },
  {
    "objectID": "radcat.html#lets-get-started",
    "href": "radcat.html#lets-get-started",
    "title": "18  R.A.D. C.A.T.",
    "section": "",
    "text": "Before beginning, when planning your work, have you…\n\n\noutlined the complete set of tasks and any intermediate deadlines for those tasks as well as any task dependencies—items you need to complete and possibly have approved before you can start or complete other tasks?\ncreated any accounts you will need at external systems and confirmed that you can log into those accounts?\ndetermined where you will save your files and developed a plan for accessing those files from whatever computer(s) you’ll be using?\nnoted any explanations, justifications, or documentation that must be provided?\nreviewed expectations for citing sources, collaboration, or use of generative AI?\nidentified any instructions about specific techniques you should be using?\nidentified any instructions about specific techniques you should not be using?\ndeveloped a personal schedule for doing the work which accounts for potentially getting stuck and needing help from your professor or PAL?\n\n\nBefore completing your work, have you…\n\n\nreviewed it to ensure it meets all of the assignment expectations you identified?\ntested any code in multiple contexts/on multiple inputs?\nreflected on your design choices?\nproofread any written content for spelling, grammar, and clarity?\nconsidered visiting the Writing Center for help revising your text?\n\n\nBefore submitting your work, have you checked for details about…\n\n\nwhere to include your name?\nwhat filename(s) to use?\nwhat file type(s) to use?\nlength or size expectations for the assignment?\nwhere/how to submit your assignment?\nthe assignment deadline and any late policies?",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>R.A.D. C.A.T.</span>"
    ]
  },
  {
    "objectID": "sample-final-project.html",
    "href": "sample-final-project.html",
    "title": "19  Work Acquistion and Gender in the Museum of Modern Art",
    "section": "",
    "text": "20 Introduction\nNote to students: this sample final project includes only writing and output without code. You can export your HTML file in the usual way, but if you’d like to produce something in this format I can work with you on that. I made this file more readable using the conversion tool Quarto, but again you can simply use the standard JupyterHub HTML export if you prefer.\nOpened in 1929, the Museum of Modern Art (MoMA) in New York City has collected modern and contemporary artworks of all kinds for nearly a century. The museum makes their collection data freely available on Github, and in this project I will use the Artworks.csv data to investigate the 140,848 artworks that MoMA data make available. This is only collection data: the dataset does not include images or other direct depictions of the artworks themselves, instead this dataset contains metadata about the art and the people who made it. As they make clear on their Github page, this data is provided “as is” and may include errors or incomplete information. Any alterations I’ve made to the data are my own, and the analysis is not intended to represent MoMA the institution in any way, in accordance with their data sharing policy. This data is a very small piece of art history: I am investigating a subset of the collection of a single museum. While this analysis may suggest some larger questions about the history of art and museum collections, it can ultimately only answer questions about MoMA itself.\nThere are many stakeholders to this data, including the museum curators, academic art historians, past and future artists, and the general public. From the lens of Positionality, we might consider that art history has typically been dominated by the stories of male artists, while women artists have been more marginalized. Understanding gender in this data could either help to address this imbalance or (if we’re not careful in the analysis) reinforce it. Considering things from the lens of Power, it’s important to remember that this is data from just one very wealthy museum in New York City. Drawing conclusions about all of modern art from just this data set could potentially mislead readers and give more power to a small set of curators and art patrons.\nIn this notebook I will look into questions about gender in MoMA’s collection. Are there more women artists or men in MoMA’s collection? Does this gender breakdown continue over time? Using the artworks’ attributes, is it possible to predict the gender for the artists? If it is, what features are most useful for predicting gender, and why? I am expecting, based on what I already know about art history, for their to be more men than women in this dataset, but I am curious if the collection practices of MoMA have gotten more balanced in recent years.",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Work Acquistion and Gender in the Museum of Modern Art</span>"
    ]
  },
  {
    "objectID": "sample-final-project.html#hypothesis-test",
    "href": "sample-final-project.html#hypothesis-test",
    "title": "19  Work Acquistion and Gender in the Museum of Modern Art",
    "section": "22.1 Hypothesis Test",
    "text": "22.1 Hypothesis Test\nNow that we’ve identified some differences between when MoMA acquired artworks by men and women, we might want to know if this difference is statistically significant. As I mentioned previously, this data contains only a subset of the artworks available in MoMA’s collection, and we further limited that dataset above by looking at only single-artist works. Are the differences we see between men and women in this data the result of randomness in our sample, or is it likely that MoMA’s collection overall has more works by women acquired more frequently.\nWe can use a permutation test to compare the mean acquired year for men and women in the MoMA data. To do so, we’ll need to define a null and alternative hypothesis:\n\\(H_0: mean(AcquiredYearMale) = mean(AcquiredYearFemale)\\)\n\\(H_1: mean(AcquiredYearMale) &lt; mean(AcquiredYearFemale)\\)\nMy null hypothesis states that there is no difference between the average year that works by men were acquired and the average year that works by women were acquired. My alternative hypothesis states that on average works by men were acquired earlier than works by women. This is a one-tailed alternative hypothesis, meaning that I am only testing in one direction. To understand why I made that choice, we can plot the difference in distributions between men and women.\n\n\n\n\n\n\n\n\n\nThis box plot shows the difference in the distributions of when artworks by men and women were acquired by the museum. The large difference between the two median lines and the different locations of the two boxes indicates that artworks by women were generally acquired more recently. Since a permutation test compares the means, we can also view this difference as a bar plot.\n\n\n\n\n\n\n\n\n\nThis bar plot shows a comparison of the means instead of the median and interquartile ranges. We can see that though the medians are more different, the means of these two groups are much closer together (indicating that some outliers may be skewing the results). However, we see very small or nonexistent confidence interval error bars on this plot. This could mean that the difference in means is statistically significant, but we will need to run a permutation test to be sure.\nSince the female acquired year average is higher (i.e. more recent) on the both the box plot and the bar plot, I’ve decided to make this a one-tailed hypothesis test. I am trying to find out: are artworks by women really acquired more recently?\n\n\n17.935649315104456\n\n\nNow that I’ve determined the guidelines for my test I can find the observed difference in means. As you can see in the code above, on average works by women are acquired 17 or 18 years later than works by men. The codeblock below creates a function for reshuffling data to create 5000 random permutations. The distribution of these permutation means are then compared to the observed difference above.\n\n\n\n\n\n\n\n\n\nThe plot above shows the permutation distribution (in white) and the observed difference in means (as a red dotted line). The permutations almost never showed a difference greater than 1 year or less than -1 year. The red line is very far way from all of the permuation data, with the entire distribution less than the observed difference in means. To complete this interpretation, we will need to calculate a p-value.\n\n\n0.0\n\n\nBy comparing the permutation data to the observed difference, we can calculate a p-value of 0.0, which is far less than the expected alpha of 0.05. In this case, \\(p&lt;0.05\\), meaning that our observed difference in means of 17 years is statistically significant. It is very unlikely to achieve this same result with a random assortment of artworks. In addition to this, though a difference of 17 years is fewer than we might expect from the exploratory data analysis above, this is also a practically significant result: 17 years is a relatively long amount of time.\nOverall, this hypothesis test has helped us to infer what the full population of data might look like. In order to understand further, it’s equally useful to use this data in a statistical model.",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Work Acquistion and Gender in the Museum of Modern Art</span>"
    ]
  },
  {
    "objectID": "sample-final-project.html#statistical-modeling-with-random-forest",
    "href": "sample-final-project.html#statistical-modeling-with-random-forest",
    "title": "19  Work Acquistion and Gender in the Museum of Modern Art",
    "section": "22.2 Statistical Modeling with Random Forest",
    "text": "22.2 Statistical Modeling with Random Forest\nGiven that the museum’s practices around art made by men and women are so different, we might learn more by using a statistical model to predict an artists’ gender based on the attributes of their artworks. Because I am predicting gender categories, I will need to use a classification model. I will attempt to use a few properties of both the artist and the artworks: the nationality of the artist, the year of their birth, the artwork’s height and width, the year it was made, and the year it was acquired by MoMA. (Note that here I need to use the “MadeYear” and “AcquiredYear” numerical variables that I created in my data wrangling section.) Because this is a mixed set of predictors, both numerical and categorical, I have a lot of choices. But since date-related data is more discrete than continuous, I have chosen a model type, the Random Forest, that deals well with a mix of numerical and categorical variables.\nTo begin, I will use a single decision tree to determine if I have chosen my predictors well.\n\n\n\n\n\n\n\n\n\nThis decision tree begins by splitting on the year the artwork was required. On the left side of the tree, when splitting by nationality, the tree struggles to identify any female artists. The model has more success on the right side of the tree, when splitting by date. This suggests that some predictors (features) are better than others. We can confirm this by looking at the feature importances for this tree.\n\n\nBeginYear 0.35232213487884184\nHeight (cm) 0.0\nWidth (cm) 0.0\nMadeYear 0.1075948726810155\nAcquiredYear 0.3944539331608834\nNationality_Albanian 0.0\nNationality_Algerian 0.0\nNationality_American 0.113986571086963\nNationality_Argentine 0.0\nNationality_Australian 0.0\nNationality_Austrian 0.0\nNationality_Azerbaijani 0.0\nNationality_Bahamian 0.0\nNationality_Belgian 0.0\nNationality_Beninese 0.0\nNationality_Bolivian 0.0\nNationality_Bosnian 0.0\nNationality_Brazilian 0.0\nNationality_British 0.0\nNationality_Bulgarian 0.0\nNationality_Burkinabé 0.0\nNationality_Cameroonian 0.0\nNationality_Canadian 0.0\nNationality_Canadian Inuit 0.0\nNationality_Chilean 0.0\nNationality_Chinese 0.0\nNationality_Colombian 0.0\nNationality_Congolese 0.0\nNationality_Costa Rican 0.0\nNationality_Croatian 0.0\nNationality_Cuban 0.0\nNationality_Czech 0.0\nNationality_Danish 0.0\nNationality_Dutch 0.0\nNationality_Ecuadorian 0.0\nNationality_Egyptian 0.0\nNationality_Emirati 0.0\nNationality_Estonian 0.0\nNationality_Ethiopian 0.0\nNationality_Filipino 0.0\nNationality_Finnish 0.0\nNationality_French 0.0\nNationality_Georgian 0.0\nNationality_German 0.0\nNationality_Ghanaian 0.0\nNationality_Greek 0.0\nNationality_Guatemalan 0.0\nNationality_Haitian 0.0\nNationality_Hungarian 0.0\nNationality_Hunkpapa Lakota 0.0\nNationality_Icelandic 0.0\nNationality_Indian 0.0\nNationality_Indonesian 0.0\nNationality_Iranian 0.0\nNationality_Iraqi 0.0\nNationality_Irish 0.0\nNationality_Israeli 0.0\nNationality_Italian 0.0\nNationality_Ivatan 0.0\nNationality_Ivorian 0.0\nNationality_Jamaican American 0.0\nNationality_Japanese 0.0\nNationality_Kenyan 0.0\nNationality_Korean 0.0\nNationality_Kuwaiti 0.0\nNationality_Latvian 0.0\nNationality_Lebanese 0.0\nNationality_Lithuanian 0.0\nNationality_Luxembourger 0.0\nNationality_Macedonian 0.0\nNationality_Malaysian 0.0\nNationality_Malian 0.0\nNationality_Mexican 0.0\nNationality_Moroccan 0.0\nNationality_Mozambican 0.0\nNationality_Namibian 0.0\nNationality_Nationality unknown 0.0\nNationality_Native American 0.0\nNationality_Nepali 0.0\nNationality_New Zealander 0.0\nNationality_Nicaraguan 0.0\nNationality_Nigerian 0.0\nNationality_Norwegian 0.0\nNationality_Oneida 0.0\nNationality_Pakistani 0.0\nNationality_Panamanian 0.0\nNationality_Paraguayan 0.0\nNationality_Peruvian 0.0\nNationality_Polish 0.0\nNationality_Portuguese 0.0\nNationality_Puerto Rican 0.0\nNationality_Romanian 0.0\nNationality_Russian 0.03164248819229636\nNationality_Salvadoran 0.0\nNationality_Scottish 0.0\nNationality_Senegalese 0.0\nNationality_Serbian 0.0\nNationality_Sierra Leonean 0.0\nNationality_Singaporean 0.0\nNationality_Slovak 0.0\nNationality_Slovenian 0.0\nNationality_South African 0.0\nNationality_South Korean 0.0\nNationality_Spanish 0.0\nNationality_Sri Lankan 0.0\nNationality_Sudanese 0.0\nNationality_Swedish 0.0\nNationality_Swiss 0.0\nNationality_Syrian 0.0\nNationality_Taiwanese 0.0\nNationality_Tanzanian 0.0\nNationality_Thai 0.0\nNationality_Turkish 0.0\nNationality_Ugandan 0.0\nNationality_Ukrainian 0.0\nNationality_Uruguayan 0.0\nNationality_Venezuelan 0.0\nNationality_Welsh 0.0\nNationality_Zimbabwean 0.0\n\n\nThe list above shows all the feature importances for the predictors we chose (with the categorical predictor nationality split into its individual dummy variables). The list suggests that the dimension of an artwork (height and width) as well as the nationality of the artist aren’t especially predictive of the artists’ gender in this model. This is a little surprising based on what we discovered about the height of artworks in our exploratory section, but it also logically follows that the gender of the artist wouldn’t affect the shape of the artwork, nor would it have any affect on their nationalities.\nHowever, there’s a problem with this model based on something we already observed in the exploratory section!\n\n\nGender\nmale      120979\nfemale     19608\nName: count, dtype: int64\n\n\nThere are more than 5 times as many male artists in our data than female artists! This will create a rare class problem: any model built on this data will get high accuracy simply by guessing the artist is male every time. To remedy this, I will include some oversampling of the female artists’ data in my training set. Note: this oversampling would typically only occur in the training set, but I have oversampled both the training and test set to simplify the code. This approach will use bootstrap sampling to create a large set of female artists to identify.\n\n\nGender\nmale      111992\nfemale    111992\nName: count, dtype: int64\n\n\nNow we’re ready to continue our modeling code. This will be more reliable than my original model, which had very high precision but low recall.\n\n\nRandomForestClassifier(max_leaf_nodes=20, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_leaf_nodes=20, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0.230055\n0.769945\n\n\n1\n0.368141\n0.631859\n\n\n2\n0.112075\n0.887925\n\n\n3\n0.130142\n0.869858\n\n\n4\n0.566794\n0.433206\n\n\n...\n...\n...\n\n\n67191\n0.578679\n0.421321\n\n\n67192\n0.785041\n0.214959\n\n\n67193\n0.446660\n0.553340\n\n\n67194\n0.092276\n0.907724\n\n\n67195\n0.315962\n0.684038\n\n\n\n\n67196 rows × 2 columns\n\n\n\nAfter setting up the random forest classifier and examining the predicted probabilities above, I am reasonably confident in this model. While some of the probabilites show about a 50-50 split in the model’s certainty that an artist is a man or a woman, a lot of the visible probabilities in this list are 60% and above for one of the two categories. This may indicate that our validation (below) will show an accurate model.\n\n\nBeginYear 0.3626260091372285\nMadeYear 0.290751730703907\nAcquiredYear 0.34662226015886444\n\n\nAs I suggested previously, reducing to just these three predictors has left us with relatively high feature importances across the board. This tells us a lot about our data, since it suggests that it’s possible to predict the gender of the artist whose work is in MoMA based only on three dates: when the artist was born, when their artwork was made, and when the artwork was acquired. There’s clearly a pattern in MoMA’s collection practices.\n\n\n\n\n\n\n\n\n\nThe confusing matrix above shows the predicted responses for all the data in our test set. As we observed in the predicted probabilities, the model seemed to do a decent job at predicting artists of both genders. There are many more true positives (in the top left) and true negatives (in bottom right) than there are false positives and false negatives. However, there are still a lot of false predictions (more than 15,000), indicating that our model could have even better performance.\n\n\n              precision    recall  f1-score   support\n\n      female       0.76      0.78      0.77     33571\n        male       0.77      0.75      0.76     33625\n\n    accuracy                           0.76     67196\n   macro avg       0.76      0.76      0.76     67196\nweighted avg       0.76      0.76      0.76     67196\n\n\n\nIn the classfication report above, you can see that overall our model is \\(76\\%\\) accurate, with similar precision for both categories (\\(75\\%\\) for women and \\(76\\%\\) for men), and similar recall as well (\\(77\\%\\) for women and \\(75\\) for men). This is much better than my original model before dealing with the rare class problem, which had precision for men in the 90s but recall only around \\(10\\%\\).\n\n\n\n\n\n\n\n\n\nBecause this is a binary classifier, we can also create an ROC curve. The ROC curve above confirms the overall accuracy of the model: there is a consistent curved line that takes up most of the graph. Area under the curve (AUC) is \\(85\\%\\), which indicates that the true positive rate for our model is higher than the false positive rate. The last thing to determine is whether our model’s accuracy in this split of the data will remain at ~\\(75\\%\\) after cross-validation.\n\n\n0.7 accuracy with standard deviation 0.11\n\n\nIn the code above I ran 5-fold cross-validation, splitting the data differently each time to reserve different data for testing. Across these runs, I got an average of \\(70\\%\\) accuracy, which is not as high as my original run but not bad for a first attempt at building a classifier. The standard deviation was a moderate \\(0.11\\), suggesting that there’s some variance in the results that might be reduced in a better model.",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Work Acquistion and Gender in the Museum of Modern Art</span>"
    ]
  },
  {
    "objectID": "guides/jupyter.html",
    "href": "guides/jupyter.html",
    "title": "20  Advanced Jupyter Setup",
    "section": "",
    "text": "20.1 Getting Set Up On Your Own Computer\nThis page will help you make sure you are set up to use Python and Jupyter Lab on your own computer as well as in the classroom. You will use the Jupyter Notebook file getting_started.ipynb linked below and also available in the Python folder in the Resources tab to test that you are set up properly.\nhttps://jrladd.com/CIS241/resources/00_getting_started.ipynb\nTo use python on your own computer, you will install Anaconda, available at https://www.anaconda.com/products/individual",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Advanced Jupyter Setup</span>"
    ]
  },
  {
    "objectID": "guides/jupyter.html#getting-set-up-on-your-own-computer",
    "href": "guides/jupyter.html#getting-set-up-on-your-own-computer",
    "title": "20  Advanced Jupyter Setup",
    "section": "",
    "text": "Download the Individual Edition installer for your system\nRun the installer using the default/recommended options\nDownload the getting_started.ipynb file linked above, keeping track of where you have saved it on your computer\nLaunch the Jupyter Lab application; this may be inside an Anaconda3 folder or an Anaconda Navigator application\nIn the interface that appears, navigate through the directories/folders on your computer to the space where you have saved the getting_started.ipynb file.\nClick on the getting_started.ipynb file to open it; you should see a collection of Python code and markdown comments, broken up between a number of boxes/cells\nFollow the instructions in the file to run the code and review the output",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Advanced Jupyter Setup</span>"
    ]
  },
  {
    "objectID": "guides/jupyter.html#getting-set-up-in-the-classroom",
    "href": "guides/jupyter.html#getting-set-up-in-the-classroom",
    "title": "20  Advanced Jupyter Setup",
    "section": "20.2 Getting Set Up In The Classroom",
    "text": "20.2 Getting Set Up In The Classroom\nImportant Note: while we will be using the same software, Jupyter Lab, in both the classroom and on your own computer, you will start the applications in different ways depending on which system you are on. Specifically, in the classroom, you will use a Jupyter.bat file to launch the software. You SHOULD NOT put this file on your own computer. On your own computer, you will launch Anaconda and Jupyter Lab as you would any other application you have installed.\nTo use python in the classroom, we will use the Anaconda package already installed on these computers:\n\nCreate a folder in your share space (that is, your H: drive) with a name like “python” or “CIS241”.\nDownload and save the getting_started.ipynb file linked above to that folder\nDownload the Jupyter.txt file linked here and save it to the desktop\nRename the Jupyter.txt file to Jupyter.bat\nDouble click the Jupyter.bat icon to launch Jupyter Lab\nIn the interface that appears, navigate to the folder you created with the getting_started.ipynb file in it\nClick on the getting_started.ipynb file to open it; you should see a collection of Python code and markdown comments, broken up between a number of boxes/cells\nFollow the instructions in the file to run the code and review the output",
    "crumbs": [
      "Home",
      "Other Key Info",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Advanced Jupyter Setup</span>"
    ]
  }
]